<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>learning on InCenger Blog</title>
    <link>https://incenger.github.io/tags/learning/</link>
    <description>Recent content in learning on InCenger Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Sun, 04 Jul 2021 11:43:10 +0700</lastBuildDate><atom:link href="https://incenger.github.io/tags/learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Efficient sub-pixel convolution</title>
      <link>https://incenger.github.io/post/learn/efficient_subpixel_conv/</link>
      <pubDate>Sun, 04 Jul 2021 11:43:10 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/learn/efficient_subpixel_conv/</guid>
      <description>
        
          &lt;p&gt;The efficient sub-pixel convolution is proposed in &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and explained in details in &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. IMHO, the explanation is &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; is very difficult to follow because the way sub-pixel convolution (or &lt;a href=&#34;https://incenger.github.io/post/learn/transposed_conv/&#34;&gt;Transpose Convolution&lt;/a&gt;) is performed in that paper is very confusing.&lt;/p&gt;
&lt;p&gt;In short, the efficient sub-pixel convolution is just a more efficient way to do sub-pixel convolution. Its superiority lies in the &lt;strong&gt;efficiency&lt;/strong&gt;, &lt;strong&gt;not the upsampling quality&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In order to understand how this works, you need to have basic understanding of Transposed convolution. We will use an example of upsampling a \(4\times 4\) feature map to a \(8\times 8\) to see the equivalence between the normal transposed convolution and the efficient sub-pixel convolution  and how the latter method is more efficient.&lt;/p&gt;
&lt;p&gt;Considering a normal convolution with kernel size \(k = 4\), stride \(s = 2\) (stride \(2\) because we want to double the spatial size of the feature map) and padding \(1\). Applying this convolution on a \(8 \times 8\) input results in a \(4 \times 4\)  output feature map. Its corresponding transposed convolution has kernel size \(k&#39; = 4\), stride \(s&#39; = 1\) with padding \(p&#39; = 2\) (The formulation of transposed convolution&amp;rsquo;s output size can be found in &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;), producing \(8 \times 8\) from \(4 \times 4\) input.&lt;/p&gt;
&lt;p&gt;Since the stride \(s = 2\), in addition to padding values around the transposed convolution input, \(s - 1\) zero pixels are added between actual pixels. The transposed convolution&amp;rsquo;s input, after inserting zeros, is: (Green circles are actual pixels, blue are zero padding)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/efficient_subpixel/padded_input.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we use a \(4 \times 4\) kernel and slide on this feature map to perform convolution. While doing that, noticing the set of weighs in the kernel that are activated at the same time, you can see the pattern. Weights at position at having the same number will be activated -  multiplying with actual pixels - at the same time (For some position at the corner of the feature map, it&amp;rsquo;s possible that only 2 of 4 weight position are activated but it doesn&amp;rsquo;t affect the generalization). While those weights are activated, weights at other position in the kernel receive zero values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/efficient_subpixel/pattern.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This observation suggests that it&amp;rsquo;s possible to break down the \(4 \times 4\) kernel into four \(2 \times 2\) kernel using the observed pattern. The transposed convolution, therefore, is equivalent to apply four \(2 \times 2\) convolution on the initial feature map (without zero values inserted between actual pixels), followed by rearranging pixels in result feature maps to create the \(8 \times 8\) output. This is exactly what the &lt;strong&gt;efficient sub-pixel convolution&lt;/strong&gt; does. The efficiency comes from doing the convolution with feature map with smaller spatial size and avoiding inserting zeros between pixels.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/efficient_subpixel/decompose.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Is the deconvolution layer the same as a convolutional layer?&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A guide to convolution arithmetic for deep learning&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

        
      </description>
    </item>
    
    <item>
      <title>Tranposed Convolution</title>
      <link>https://incenger.github.io/post/learn/transposed_conv/</link>
      <pubDate>Sun, 04 Jul 2021 11:34:13 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/learn/transposed_conv/</guid>
      <description>
        
          &lt;p&gt;The transposed convolution is also called:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fractionally strided convolution&lt;/li&gt;
&lt;li&gt;Sub-pixel convolution&lt;/li&gt;
&lt;li&gt;Backward convolution&lt;/li&gt;
&lt;li&gt;Deconvolution (The worst name -  it&amp;rsquo;s confused and shouldn&amp;rsquo;t be used at all)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we unroll the input and output feature maps of a convolution operation into vectors from left to right, top to bottom, the convolution could be represented by a matrix multiplication with a spare matrix \(C\). For example, below is a spare matrix for a convolution with kernel size \(3\), stride \(1\), padding \(0\) over a \(4 \times 4\) input, resulting in a \(2\times2\) output.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/transposed_conv/conv_matrix.png&#34; alt=&#34;Matrix representation of convolution&#34;&gt;&lt;/p&gt;
&lt;p&gt;The convolution above transformed the \(16\)-dimensional vector into \(4\)-dimensional vector. With this representation, it&amp;rsquo;s easy that we can do the reverse transform, which is transforming a \(4\)-dimensional input to \(16\)-dimensional input while keeping the original connectivity pattern, with the help of the transposed matrix \(C^T\). This is the underlying principal of the transposed convolution -  swapping the forward and backward pass of the normal convolution.&lt;/p&gt;
&lt;p&gt;One way to think about transposed convolution is by multiplying the scalar in the input feature map with the kernel weights to create immediate results of the output feature map. The position of the immediate result depends on the position of the scale in the input feature map. The final output is the sum of all immediate results.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/transposed_conv/transposed_conv_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another way to think about transposed convolution is to apply an equivalent - but much less efficient - direction convolution on input feature map. Imagining the input of transposed convolution being the result of a direction convolution applied on some initial feature map, the transposed convolution can be considered as the operation that recovers the shape of this initial feature map.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; This way of thinking is reasonable because the transposed convolution is the just backward pass of a direction convolution.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/transposed_conv/transposed_conv_2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In addition, considering transposed convolution in its relation to a direction convolution gives rise to the relationship between two convolution operations. Given a direct convolution with kernel size \(k\), stride \(s\), padding \(p\), the transposed convolution has the same kernel size \(k&#39; = k\), stride \(s&#39; = 1\) (noted that the stride for convolution of transposed convolution always has stride \(1\)).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When \(p = 0\) (no padding) and \(s = 1\), considering the top left pixel of input of normal convolution, it only contributes to the top left pixel to the output. Therefore, for transposed convolution to maintain this connectivity pattern, it&amp;rsquo;s necessary to add zero padding around the input of transposed convolution. The padding for transposed convolution&amp;rsquo;s input is \(p&#39;=k-1\).&lt;/li&gt;
&lt;li&gt;When padding is used in the normal convolution, it&amp;rsquo;s reasonable to see that we need to reduce the padding of the transposed convolution. This can be explained by the observation that the top-left pixel of the original input (before padded) now contributes to more pixels output. The new padding for transposed convolution&amp;rsquo;s input is \(p&#39; = k - 1 -p\)&lt;/li&gt;
&lt;li&gt;When stride \(s &amp;gt; 1\) is used for normal convolution, the intuition for transposed convolution is to make the sliding window moves slower. This is made possible by adding zero pixels &lt;strong&gt;between&lt;/strong&gt; actual pixel in the input. For normal convolution with stride \(s\), \(s-1\) zero pixels are added between actual in the input. This is where the name &lt;em&gt;fractionally strided convolution&lt;/em&gt; comes from. Strided transposed convolution is commonly used to  increase the spatial size of feature map.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/transposed_conv/transposed_conv_3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A guide to convolution arithmetic for deep learning&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://d2l.ai/chapter_computer-vision/transposed-conv.html&#34;&gt;https://d2l.ai/chapter_computer-vision/transposed-conv.html&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

        
      </description>
    </item>
    
    <item>
      <title>Deformable Alignment</title>
      <link>https://incenger.github.io/post/learn/deformable_alignment/</link>
      <pubDate>Sat, 03 Jul 2021 12:12:52 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/learn/deformable_alignment/</guid>
      <description>
        
          &lt;p&gt;Motion is an intrinsic part of video. It poses a question about how to extract, exploit, and encode motion information when working with video data. When adapting methods for single image for video, it&amp;rsquo;s usually useful to leverage the prior knowledge that frames of the same scene can be approximated by a reference frame and motion information. The motion between frames can be expressed in many forms, but the most natural and intuitive choice is the optical flow - the displacement of pixels. When it comes to Video Super Resolution (VSR), the motion problem can be decomposed into two questions:how to use the temporal information to constrain the problem and how to effectively extract and exploit temporal correspondence. The former question originates from the ill-posed nature of the Video Super Resolution problem. The later question arises from the fact that adjacent frames of the target one are accessible. Developing alignment algorithms that can adaptive extract motion feature and have the capacity to respond with sudden and complex motions is one of the primary goal in VSR problems.&lt;/p&gt;
&lt;p&gt;Early approaches&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;  &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; to exploit motion information in VSR problems can be decomposed into two stages: motion estimation and motion compensation. In the first stage, dense optical flow between reference and target frames are extracted using flow estimation algorithm. In the second stage, reference frames are warped using estimated flow from the first stage, resulting in compensated or aligned frames. Even though motion estimation and compensation are usually performed on frames, it&amp;rsquo;s possible to perform those on feature maps. Those compensated frames or feature maps should encode motion feature that succeeding algorithms in the VSR network can exploit.  Despite benefiting from the development of flow estimation algorithms, integrating state-of-the-art flow estimation algorithms into VSR network is not simply plug-and-play. If the flow estimation network is used offline - that is to generate the optical flow outside of the VSR network, the motion estimation quality is not good enough due to data limitation. On the other hand, it&amp;rsquo;s challenging to train the flow network and VSR network in the end-to-end manner since two network differs in training data and optimization process.&lt;/p&gt;
&lt;p&gt;In order to overcome the limitations of flow-based alignment approach, recent studies have proposed an implicit alignment method using &lt;a href=&#34;https://incenger.github.io/post/learn/deformable_conv/&#34;&gt;Deformable Convolution&lt;/a&gt; &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.  The deformable alignment has the merit of being both effective and trainable in end-to-end manner. Given the reference and neighbor frame (or feature maps), deformable alignment applies deformable convolution on the neighboring frame to align it to the reference frame. The offset for the deformable convolution is learned from both the target and reference features, for example by applying a few convolution layers on the concatenation of the two features.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/deformable_alignment/deformable_alignment.png&#34; alt=&#34;Deformable Alignment &#34;&gt;&lt;/p&gt;
&lt;p&gt;How this method works is extensively studied in &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;. In specific, let \(x\) be the input feature, \(y\) is the output of the deformable convolution on \(x\), \(p_k\) and \(\Delta_{k}\) is the \(k\)-the sampling position and its corresponding offset in the sampling grid \(G\) for output position \(p\), we have:&lt;/p&gt;
&lt;p&gt;$$ y = \sum_{k=1}^{|G|} w(p_k) \cdot x \left(p + p_k + \Delta_{k}\right) $$&lt;/p&gt;
&lt;p&gt;The part \(p + p_k + \Delta_{k}\) can be viewed as displacing or warping the pixel at location \(p\) by a displacement of \(p_k + \Delta_{k}\), which is equivalent to image warping in flow-based alignment. The deformable convolution with kernel size \(|G|\), therefore, are capable of generating \(|G|\) warped feature maps, which are later aggregated using a \(1 \times 1\) convolution via the kernel weight \(w\).
In the special case where \(|G| = 1\), the deformable alignment is equivalent to a spatial warping followed by a \(1 \times 1\) convolution - similar to what we want achieve with flow-based alignment. This observation also points out the advantage of deformable alignment: offset diversity - the capacity to generate more than one potential displacement for each pixel. In other words, deformable alignment and flow-based alignment share their same underpinnings with small difference in the offset-diversity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/deformable_alignment/deformable_convolution_decompose.png&#34; alt=&#34;Decomposing Deformable Convolution &#34;&gt;&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Frame Recurrent Video Super-Resolution&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Detail revealing Deep Video Super-resolution&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Temporally Deformable Alignment Network for Video Super-Resolution&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;EDVR: Video Restoration With Enhanced Deformable Convolutional Networks&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Understanding Deformable Alignment in Video Super-Resolution&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

        
      </description>
    </item>
    
    <item>
      <title>Deformable Convolution</title>
      <link>https://incenger.github.io/post/learn/deformable_conv/</link>
      <pubDate>Fri, 02 Jul 2021 23:14:09 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/learn/deformable_conv/</guid>
      <description>
        
          &lt;p&gt;In &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, the authors argue that the traditional CNNs are inherently limited to model geometric transformation due to the fixed geometric structures in its building components. Most of popular building blocks of a CNN such as convolution layer, max-pooling, and region of interest pooling all have fixed geometric structures. The convolution layer samples from the input feature map at fixed locations; the max pooling layer reduces the spatial resolution at a fixed rates; and the output feature map of the ROI pooling has fixed no matter the size of the input feature map. As a result, a CNN built on these layers inherently has similar limitation. For example, all activation units in one layer of a CNN could have the same receptive field, which is undesirable especially for high level convolution layer that captures semantic features. This limitation causes CNNs to be less effective in a few visual recognition tasks requiring fine localization such as semantic segmentation as these tasks necessitates the capacity to respond with different object scales and deformation with adaptive receptive fields.&lt;/p&gt;
&lt;p&gt;The adaptiveness of deformable convolution lies in its augmenting offset grids. In specific, considering a traditional 2D convolution operation that can be broken down into two steps: 1 - sampling from the input feature map \( \mathcal{f_{in}} \)  using a rectangular grid \( \mathcal{R} \) and weighted summation of those sampled values using a learnable weight \( \mathcal{w} \). In plain  \( 3 \times 3  \) convolution with stride $1$, the sample grid \( \mathcal{G} \) is:&lt;/p&gt;
&lt;p&gt;$$\mathcal{G} = { (-1, -1), \dots, (0, 0), \dots, (1,1)}$$&lt;/p&gt;
&lt;p&gt;Values in the output feature map \( \mathcal{f_{out}} \) at location \( p \) is defined by:&lt;/p&gt;
&lt;p&gt;$$\mathcal{f_{out}}(p) = \sum_{\mathcal{g} \in \mathcal{G}} w(g) \cdot \mathcal{f}_{in}(p + g)$$&lt;/p&gt;
&lt;p&gt;Deformable convolution augments the sampling grid  \( \mathcal{G} \) using an offset grid \( \mathcal{O} \) having the same size as \( \mathcal{G} \) , resulting in a sampling offset grid \( \mathcal{G&#39;} = \mathcal{G} + \mathcal{O} \).  In succeeding work, a modulated mask \( \mathcal{M} \) is integrated into deformable convolution to further strengthen its capacity. With values lie in the range \( [0, 1] \), the modulated mask allows deformable convolution to suppress information from some particular position in input feature map. The deformable convolution with modulated mask can be expressed as:&lt;/p&gt;
&lt;p&gt;$$\mathcal{f_{out}}(p) = \sum_{i = 1}^{|\mathcal{G}|} w(\mathcal{G}_{i}) \cdot \mathcal{f}_{in}(p + \mathcal{G&#39;}_{i}) \cdot \mathcal{M_{i}} $$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/deformable_convolution/deformable_conv.png&#34; alt=&#34;Visual Explanation of Deformable Convolution&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Deformable Convolutional Networks&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Deformable convnets v2: More deformable, better results&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
