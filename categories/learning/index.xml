<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>learning on Hi! I'm Tam Le</title><link>https://incenger.github.io/categories/learning/</link><description>Recent content in learning on Hi! I'm Tam Le</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>&lt;a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0&lt;/a></copyright><lastBuildDate>Sun, 04 Jul 2021 11:34:13 +0700</lastBuildDate><atom:link href="https://incenger.github.io/categories/learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Tranposed Convolution</title><link>https://incenger.github.io/posts/2021/07/transposed_conv/</link><pubDate>Sun, 04 Jul 2021 11:34:13 +0700</pubDate><guid>https://incenger.github.io/posts/2021/07/transposed_conv/</guid><description>The transposed convolution is also called:
Fractionally strided convolution Sub-pixel convolution Backward convolution Deconvolution (The worst name - it&amp;rsquo;s confused and shouldn&amp;rsquo;t be used at all) If we unroll the input and output feature maps of a convolution operation into vectors from left to right, top to bottom, the convolution could be represented by a matrix multiplication with a spare matrix \(C\). For example, below is a spare matrix for a convolution with kernel size \(3\), stride \(1\), padding \(0\) over a \(4 \times 4\) input, resulting in a \(2\times2\) output.</description></item></channel></rss>