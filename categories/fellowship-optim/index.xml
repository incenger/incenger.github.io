<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fellowship-Optim on InCenger Blog</title>
    <link>https://incenger.github.io/categories/fellowship-optim/</link>
    <description>Recent content in Fellowship-Optim on InCenger Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Fri, 02 Jul 2021 23:13:34 +0700</lastBuildDate><atom:link href="https://incenger.github.io/categories/fellowship-optim/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Fellowship of the Optim: Writing Thesis</title>
      <link>https://incenger.github.io/post/fellowship_optim/thesis_writing_5/</link>
      <pubDate>Fri, 02 Jul 2021 23:13:34 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/fellowship_optim/thesis_writing_5/</guid>
      <description>
        
          &lt;p&gt;Vào thời điểm mình viết những dòng này, mình chỉ còn khoảng một tháng cho thesis của mình. Thật sự thì mình chưa bao giờ viết một văn bản khoa học nào bằng tiếng Anh dài như thesis sắp đến của mình (yêu cầu tối thiểu là 50 trang) nên mình có hơi chút lo sợ. Mình nghĩ cảm thấy lo sợ cũng là điều bình thường thôi nhỉ, vì viết một cái gì đó 50 trang thì vốn đã không phải là chuyện đơn giản, mà đây còn là viết về nghiên cứu khoa học nữa chứ. Viết blog này cũng cho mình một bài học thú vị, là trở ngại đầu tiên khi bạn muốn viết một cái gì đó nghiêm túc chính là bắt đầu. Đó là lý do tại sao mình quyết định bắt tay vào viết khi mình còn tận một tháng nữa cho thesis.&lt;/p&gt;
&lt;p&gt;Gần đây mình có đọc quyển How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking và tìm hiểu về &lt;a href=&#34;https://notes.andymatuschak.org/About_these_notes&#34;&gt;Andy&amp;rsquo;s working note&lt;/a&gt;. Mình cảm thấy incremental writing rất phù hợp với phong cách viết của mình và những gì mình hướng đến. Cụ thể là, mình xem viết như là một cách để hiểu và suy nghĩ về một vấn đề và những note mình viết phải có giá trị tái sử dụng, tức là bản thân nó có chứa đầy đủ thông tin và liên kết để làm sáng tỏ một chủ đề nào đó. Đặt trong bối cảnh thesis của mình, thì mình muốn mỗi note của mình sẽ viết một chủ đề, một câu hỏi nào đó mà mình có thể xem lại sau này. Thật ra điều này nó cũng là một sự an ủi về mặt tinh thần cho mình. Thú thật, mình cảm thấy luận văn mình không có gì đặc sắc lắm, và việc viết một văn bản 50 trang không có nhiều giá trị cho người đọc làm mình cảm thấy thật sự rất chán chường. Vậy nên, để vượt qua điều đó, mình muốn viết thesis theo hướng ít nhất là sẽ có ích cho mình, và dù không có nhiều sự mới mẻ hay ý tưởng hay ho, thì cũng sẽ có ích cho người khác bằng cách giải thích những khái niệm, trả lời những câu hỏi mà hầu như các papers cho là hiển nhiên hoặc là bỏ qua.&lt;/p&gt;
&lt;p&gt;Vòng vo như vậy đủ rồi, thật ra mình cũng muốn làm kiểu gì đó giống như livestream viết thesis. Nhưng mình nghĩ đó không phải là ý tưởng hay lắm. Một phần vì máy mình cũng không xịn lắm, không biết livestream có ảnh hưởng đến workflow của mình không. Một lý do nữa là khi tập trung thì mình muốn tập trung nên sẽ không có tương tác gì đâu (tóm lại là khi viết mình chán lắm, chả có gì để xem đâu =))) ). Vậy nên thay vì livestream, mình sẽ update tại đây tất cả những note sau khi mình viết xong. Mặc dù những note đó chưa được review cẩn thận và có thể chưa hoàn thiện 100%, nội dung cốt lõi của chúng vẫn được đảm bảo. Mình hi vọng chúng sẽ có ích với mọi người (À tất nhiên mấy notes sẽ được viết bằng tiếng Anh do chúng sẽ là một phần của thesis mình mà :)) )&lt;/p&gt;
&lt;p&gt;Thesis evergreen notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://incenger.github.io/post/learn/deformable_conv/&#34;&gt;Deformable Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://incenger.github.io/post/learn/deformable_alignment/&#34;&gt;Deformable Alignment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://incenger.github.io/post/learn/transposed_conv/&#34;&gt;Transposed Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://incenger.github.io/post/learn/efficient_subpixel_conv/&#34;&gt;Efficient Sub-pixel Convolution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Các bạn có thể tìm đọc tất cả bài viết của series này tại &lt;a href=&#34;https://incenger.github.io/categories/fellowship-optim/&#34;&gt;đây&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
        
      </description>
    </item>
    
    <item>
      <title>The OK Plateau</title>
      <link>https://incenger.github.io/post/fellowship_optim/ok_plateau_4/</link>
      <pubDate>Fri, 07 May 2021 13:52:06 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/fellowship_optim/ok_plateau_4/</guid>
      <description>
        
          &lt;p&gt;Dạo gần đây mình có đọc Moonwalking with Einstein - một quyển sách viết về những kĩ thuật cải thiện khả năng ghi nhớ thông qua hành trình của một người &amp;ldquo;bình thường&amp;rdquo; tiếp cận với những kĩ thuật này. Nhân vật chính, cũng là tác giả, từ chỗ đầy hoài nghi về những phương pháp ghi nhớ mà cụ thể là Memory Palace (hay method of loci), thử luyện tập nó trong vòng một năm và cuối cùng là vộ địch USA Memory Championship. Quyển sách này cũng sẽ cho bạn những thông tin cần thiết để hiểu hơn về việc tại sao chúng ta phải ghi nhớ trong thời đại mà con người có thể &amp;ldquo;externalize&amp;rdquo; kí ức thông qua hằng tá ứng dụng hỗ trợ. Khoan đã&amp;hellip;., mình đâu có ý định review sách đâu nhỉ ⚆ _ ⚆. Mình hơi lan man một tí, quay trở lại chủ đề chính thôi.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/ok_plateau_4/sherlock_mind.png&#34; alt=&#34;Nếu bạn đã từng coi series Sherlock Holmes, thì các bạn nhiều khả năng sẽ biết đến kĩ thuật này :)&#34;&gt;&lt;/p&gt;
&lt;p&gt;Trong lúc đọc sách, mình có gặp phải một chủ đề khá thú vị: the OK plateau. Thuật ngữ này chỉ một giai đoạn mà một người sẽ gặp phải trong quá trình tiếp nhận và học tập một kĩ năng mới. Để hiểu về giai đoạn này thì trước tiên chúng ta cần phải biết về 3 giai đoạn mà một người sẽ trải qua trong quá trình phát triển một kĩ năng:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cognitive stage: Ở giai đoạn này chúng ta sẽ bắt đầu khám phá những phương pháp khác nhau để hoàn thiện kĩ năng của mình. Đặc trưng của giai đoạn này là chúng ta sẽ mắc rất nhiều lỗi, nhưng cũng sẽ học được rất nhiều điều mới.&lt;/li&gt;
&lt;li&gt;Associative stage: Chúng ta sẽ bắt đầu quen dần và mắc ít lỗi hơn. Ở giai đoạn này, việc luyện tập đóng vai trò rất quan trọng. Luyệt tập càng nhiều, kĩ năn của chúng ta sẽ tiến bộ càng nhanh. Và khi kĩ năng của chúng ta tiến bộ, chúng ta dần có thể thực hiện kĩ năng mà không tập trung cao độ.&lt;/li&gt;
&lt;li&gt;Autonomous stage: Kĩ năng hiện tại sẽ ở mức &amp;ldquo;ổn&amp;rdquo;. Chúng ta đạt được trạng thái &amp;ldquo;tự động lái&amp;rdquo; (autopilot), nghĩa là có thể thực hiện kĩ năng mà không cần hoặc cần rất ít sự tập trung. Lúc này cũng chính là lúc mà chúng ta bước vào The OK Plateau - giai đoạn mà chúng ta dẫu dành nhiều thời gian để thực hiện kĩ năng nhưng nó sẽ hầu như không tiến bộ. ᕙ(⇀‸↼‶)ᕗ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/ok_plateau_4/ok-plataeu.png&#34; alt=&#34;Đi đâu về đâu?&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Practice makes perfect&amp;rdquo; - câu nói này đúng với hai giai đoạn đầu tiên (thật ra nó cũng sẽ đúng với giai đoạn thứ 3, mình sẽ giải thích sau). Ở những giai đoạn đó, chăm chỉ luyện tập là cách giúp bạn tiến bộ nhanh nhất.
Nhưng một khi đã đạt được giai đoạn thứ 3 và rơi vào the OK plateau, thì thời gian &amp;ldquo;luyện tập&amp;rdquo; không tỉ lệ thuận với sự tiến bộ nữa. Ví dụ điển hình cho việc này là việc học gõ phím. Khi bạn đã đạt được trạng thái có thể gõ một cách thoải mái, thì khi đó cho dù bạn dành hàng giờ đồng hồ để gõ, tốc độ của bạn cũng sẽ không cải thiện hơn là mấy. Mình có một ví dụ khác - cái này thì mình đã trực tiếp trải nghiệm được trong lúc học giải Rubik 3x3. Ba giai đoạn mà mình đã trải qua:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cognitive stage: Mình học những công thức cơ bản để giải được một khối Rubik. Mình mất khoảng vài ngày để giải xong khối đầu tiên, bằng cách vừa nhìn công thức vừa xoay. Sau đó mình bắt đầu học phương pháp CFOP (hay còn được biết đến là Fridirch method), khi đó tốc độ giải của mình cải thiện đáng kể, giảm xuống còn vài phút.&lt;/li&gt;
&lt;li&gt;Associative stage: Mình luyện tập xoay Rubik rất nhiều, những công thức dần trở thành muscle memory. Thời gian giải của mình cũng tiếp tục giảm: dưới 1 phút, rồi sau đó dưới 30 giây.&lt;/li&gt;
&lt;li&gt;Autonomous stage: Mình đã dừng lại trạng thái này được khoảng vài năm :))). Bây giờ mình có thể giải một cục rubik hoàn toàn bằng muscle memory chứ không còn nhớ được công thức nữa. Nếu bạn đứa một khối rubik cho mình vào bảo mình xoay thì mình sẽ xoay được, nhưng nếu bảo mình chỉ cho bạn thì mình chịu vì chính mình còn không nhớ rõ được là mình đã vừa xoay nó như thế nào =)))). Và tất nhiên trong giai đoạn này, thời gian mình giải không tăng lên mà còn giảm đi do tần suất mình xoay càng ngày càng ít :)))).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thật ra việc bị mắc lại ở OK Plateau không phải là một điều gì xấu. Khi bạn mắc kẹt tại đây, thì cũng là lúc mà bạn đang một cách vô thức thừa nhận với bản thân rằng mình ổn với kĩ năng hiện tại và không cần chú trọng đến việc cải thiện nó nữa. Nếu bạn thấy việc gõ với tốc độ khoảng 70 - 80 chữ là ổn thì bạn hoàn toàn có thể dừng lại ở đó và dành thời gian để tiếp tục phát triển thêm những kĩ năng khác. Mình cũng đã lựa chọn như vậy với Rubik, khi mà mình thấy việc giải nó dưới 30s đã đủ là để gây ấn tượng với hầu hết mọi người =))))&lt;/p&gt;
&lt;p&gt;Nếu bạn có để ý thì mình dùng từ &amp;ldquo;luyện tập&amp;rdquo; trong ngoặc kép khi ở the OK plateau. Ở giai đoạn này, chúng ta thường hay nhầm lẫn việc &amp;ldquo;luyện tập&amp;rdquo; (practice) và &amp;ldquo;thực hiện&amp;rdquo; (perform) kĩ năng. (Sự khác nhau giữa hai khái niệm này được giải thích rất dễ hiểu trong &lt;a href=&#34;https://www.youtube.com/watch?v=f2O6mQkFiiw&#34;&gt;video TedEd này&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) Một khi đã rơi vào trạng thái &amp;ldquo;autopilot&amp;rdquo; thì đó không còn được gọi là luyện tập nữa và bạn cần phải thay đổi phương pháp luyện tập của mình. Cụ thể hơn, bạn cần phải đưa bản thân thoát khoải trạng thái &amp;ldquo;autopilot&amp;rdquo; thông qua việc tập trung vào kĩ thuật, đặt mục tiêu cụ thể, và nhận sự phản hồi thường xuyên và ngay lập tức. Nói cách khác, bạn phải đưa mình trở lại giai đoạn &amp;ldquo;cognitive stage&amp;rdquo; - luyện tập một cách &amp;ldquo;consciously&amp;rdquo; và chấp nhận việc mắc lỗi.&lt;/p&gt;
&lt;p&gt;Mình nghĩ, điều cốt lõi trong việc vượt qua đươc the OK plateau là việc mắc lỗi và nhận phản hồi từ người khác hoặc thông qua việc tự reflection. Khi đã tích lũy được kinh nghiệm, thì chúng ta sẽ không còn mắc những lỗi &amp;ldquo;ngớ ngẩn&amp;rdquo; nhưng lúc vừa mới &amp;ldquo;chập chững&amp;rdquo; nữa. Lúc này, để mắc lỗi, chúng ta cần phải bước ra khỏi &amp;ldquo;comfort zone&amp;rdquo;, dám thử nghiệm những điều mới và dám thất bại. Mình vẫn ở trong giai đoạn cố gắng thúc đẩy bản thân làm những chuyện này. Mình vẫn chưa cảm thấy thoải mái với việc thất bại, nhất là với những dự định quan trọng. Nhưng không sao, mình tin là mình sẽ làm được, giống như lời một người chị từng bảo với mình: &amp;ldquo;Em đừng quá sợ việc thất bại. Hãy fail và fail-fast&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/ok_plateau_4/mistakes.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Các bạn có thể tìm đọc tất cả bài viết của series này tại &lt;a href=&#34;https://incenger.github.io/categories/fellowship-optim/&#34;&gt;đây&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=f2O6mQkFiiw&#34;&gt;https://www.youtube.com/watch?v=f2O6mQkFiiw&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
        
      </description>
    </item>
    
    <item>
      <title>The Fellowship of the Optim: You don&#39;t know what you don&#39;t know</title>
      <link>https://incenger.github.io/post/fellowship_optim/you_dont_know_3/</link>
      <pubDate>Sun, 02 May 2021 12:54:23 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/fellowship_optim/you_dont_know_3/</guid>
      <description>
        
          &lt;p&gt;Bỗng dưng mình có một suy nghĩ, là trong những bài viết này mình nên viết như thể mình đang kể một câu chuyện cho người đọc, hay là chỉ đơn thuần là viết để ghi lại cho bản thân mình thôi. Mình có suy nghĩ này vì với bài viết này, mình cảm giác mình muốn viết lại cho chính mình nhiều hơn là viết cho một người khác đọc. Nhưng ở hai câu vừa rồi thì mình đã chọn cách viết rồi mà nhỉ? Ừ, hơi mâu thuẫn thật, giống như trang giấy mà bạn sẽ thường hay thấy khi đọc sách này.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/you_dont_know_3/left_blank.jpg&#34; alt=&#34;Is it blank?&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You don&amp;rsquo;t know what you don&amp;rsquo;t know&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Mình thích câu này mặc dù nó luôn làm mình hơi rùng mình một tí.
Thừa nhận với chính bản thân mình rằng mình đang không biết, không hiểu rõ một điều gì đó là bước đầu tiên để chúng ta có thể bắt đầu quá trình tìm hiểu và học hỏi.
Mình không biết rất nhiều thứ, và điều đó có làm mình lo lắng một chút. Nhưng cảm giác đó, với mình, không đáng sợ bằng cảm giác nhận ra mình không biết là mình không biết về thứ gì đó.
Nói một cách đơn giản, mình nghĩ điều này thường xảy ra dưới 2 dạng: Bạn không hề nhận thức được gì về điều bạn không biết, hoặc là bạn nghĩ rằng mình đã hiểu rõ về thứ đó, nhưng thật ra góc nhìn của bạn còn đang rất hạn hẹp và thiếu sót.
Ở dạng thứ nhất, thông thường bạn sẽ nhận ra nó với một cảm giác ngạc nhiên: &amp;ldquo;Ồ, hóa ra những thứ thế này có tồn tại&amp;rdquo;.
Ở dạng thứ hai, bạn sẽ nhận ra nó với cảm giác rùng mình: &amp;ldquo;Vậy hóa ra lâu nay mình đang hiểu sai sao?&amp;rdquo;
Mình không thích dạng thứ hai, nhưng mình phải thừa nhận nó cho mình nhiều bài học hơn. Nó giống như là một lời cảnh tỉnh, để mình có thể nhìn lại tất cả những &amp;ldquo;nền móng&amp;rdquo; mà mình đang đứng trên, liệu nó đang kiên cố và vững chãi, hay đang sẵn sàng sụp đổ bất cứ lúc nào?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/you_dont_know_3/dishes_falling.jpg&#34; alt=&#34;Thế này có được coi là vững chãi không?&#34;&gt;&lt;/p&gt;
&lt;p&gt;Mình viết về câu nói này, vì gần đây mình vừa trải nghiệm nó. Để giải thích cho vấn đề này, thì mình sẽ phải nói về đề tài luận văn mình đang làm một tí. Mình làm về Video Super Resolution, nhưng để cho đơn giản, mình sẽ coi như là Single Image Super Resolution. Một khi đã hiểu được vấn đề thì các bạn có thể dễ dàng tổng quát hóa nó lên thôi.&lt;/p&gt;
&lt;p&gt;Super Resolution có thể hiểu đơn giản là phóng to ảnh, ví dụ phóng to một bức ảnh có độ phân giải 640x360 (360p) thành 1280x720 (720p) (độ phóng to - scaling factor trong trường hợp này là \( \times 2 \) ). Bài toán Super Resolution được đặt ra như sau: cho trước một cặp ảnh low resolution và high resolution, tìm một thuật toán (có thể hiểu là model nếu bạn quen với Deep Learning nhiều hơn) nhận vào ảnh low resolution và trả về kết quả giống ảnh high resolution cho trước nhất. &amp;ldquo;Giống nhất&amp;rdquo; ở đây có thể được đo bằng nhiều metrics khác nhau, đơn giản nhất thì có thể dùng Mean Square Error giữa hai ảnh.
Vấn đề đặt ra ở đây là ảnh low resolution được sinh ra từ ảnh high resolution như thế nào? Cách tiếp cận phổ biến nhất (qua những papers Super Resolution mà mình đã đọc) là sử dụng Gaussian Blur lên trên ảnh High Resolution, sau đó dùng bicubic interpolation để resize (thu nhỏ) lại và cuối cùng cộng với một Gaussian noise. Thông thường, quá trình đó được thể hiện thông qua công thức:&lt;/p&gt;
&lt;p&gt;$$I^{LR} = {(I^{HR} \circledast k) \downarrow} + \mathcal{N} $$&lt;/p&gt;
&lt;p&gt;Trong đó:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\( \circledast \) là convoluton operator&lt;/li&gt;
&lt;li&gt;\( k \) là Guassian kernel dùng để blur (cái này quan trọng, nhớ nó nhé)&lt;/li&gt;
&lt;li&gt;\(  \downarrow \) là bicubic interpolation&lt;/li&gt;
&lt;li&gt;\(  \mathcal{N} \) là Gaussian noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nói một cách đơn giản, thì Super Resolution, đặc biệt nếu dùng cách tiếp cận bằng Deep Learning models, thì sẽ cố gắng đi tìm &amp;ldquo;hàm ngược&amp;rdquo; \( k&#39; \) của kernel \( k \).
Mình đã &amp;ldquo;look over&amp;rdquo; chi tiết này, nhưng cũng nhờ vậy mà mình có được những bài học.&lt;/p&gt;
&lt;p&gt;Deep Learning models thường được ưa chuộng vì khả năng generalization của nó nếu được trained properly và mình nghĩ Super Resolution models cũng vậy. Trong lúc làm thí nghiệm, mình có một xây dựng một vài models cơ bản. Cơ bản ở đây theo nghĩa là nó đơn giản để xây dựng dựa trên một số công trình ở trước, chứ không hẳn là nó đơn giản thật =))). Mình có thử 2 cái, mà mình đặt tên là SimpleBaseline và DumbNet (đúng rồi, cái DumbNet mà mình đã viết &lt;a href=&#34;https://incenger.github.io/post/fellowship_optim/random_things_2/&#34;&gt;ở đây&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.). Kết quả train của 2 models này, trên tập validation (gồm 4 videos) của dataset REDS khá là tốt, đạt được khoảng 30 PSNR score (ngang bằng với đa số papers trong khoảng vài năm trở lại đây, nhưng tất nhiên không phải cao nhất =))) ).&lt;/p&gt;
&lt;p&gt;Với kết quả như vậy, thì việc tiếp theo mình sẽ làm là sẽ test thử kết quả trên một dataset khác. Dataset đó sẽ đóng vai trò là một out-of-scope test set (hoặc out-of-distribution, mình không nhớ chính xác tên khái niệm lắm) - một tập test mà data sẽ khác hoàn toàn với data mà model được train (khác ở đây theo nghĩ chúng có thể được sampled từ những distribution khác nhau, nếu muốn tìm hiểu thêm về phần này, mình recommend các bạn nên đọc quyển &lt;a href=&#34;https://www.manning.com/books/human-in-the-loop-machine-learning&#34;&gt;Human-in-the-Loop Machine Learning&lt;/a&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;). Mình sử dụng dataset Vid4 -  một dataset khác cũng được sử dụng khá phổ biến cho bài toán Video Super Resolution. Dataset này thì không có sẵn low resolution frame, nên mình viết code để tạo chúng từ high resolution frames. Sau khi test thử, kết quả mình nhận được rất &amp;hellip; tệ, tệ hơn hẳn việc sử dụng bicubic interpolation thông thường.  Và thế là quá trình debug của mình bắt đầu.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Để debug trong deep learning, các bạn không nên chỉ nhìn vào con số tổng quát mà hãy tập trung vào từng sample cụ thể. Mình xem kết quả super resoluton frame by frame và nhận ra là output của mình có rất nhiều artifact như thế này.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/you_dont_know_3/vid4_artifact.png&#34; alt=&#34;Kết quả rất tệ&amp;hellip;&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;Mình không thể hiểu được kết quả. Đối với các bài toán như Classification, việc hiểu kết quả đưa ra bởi model sẽ trực quan hơn nhiều. Còn với Super Resolution, mình không thật sự hiểu được hoàn toàn mà mình chỉ biết là nó tệ hay tốt thôi. Dù vậy, mình có một giả thiết là việc generalization của model có vấn đề.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mình kiểm tra lại trong paper, thì không thấy paper nào nói về chuyện này cả, và những papers đó đều có sử dụng Vid4 dataset để evaluate và kết quả khá tốt, ít nhất là tốt hơn bicubic interpolation. Mình nghĩ có thể có 2 trường hợp xảy ra: một là model của mình overfit data và hai là model đơn giản quá nên underfit. Sau khi kiểm tra kết quả trên chính training set, mình khá chắc chắn đây không thể là overfitting vì kết quả không quá tốt, do vậy mình kết luận là underfitting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mình làm thí nghiệm để xác thực giả thuyết underfitting. Mình đo số parameters và đồng thời chỉnh lại hyperparameters của model để tăng complexity lên. Sau khi train lại và evaluate lại, mình nhận kết quả tương tự :(. Và thế là mình bị mắc kẹt.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lúc này, mình đành phải kiểm chứng lại kết quả của những papers khác. Mình sử dụng code và pre-trained models của họ để chạy thử trên data Vid4 của mình. Và thật bất ngờ, kết quả cũng tệ tương tự.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tới lúc này, giả thuyết của mình chuyển từ underfitting sang việc generalization trong bài toán Super Resoluton có gì đó mà mình chưa biết (yeah, you don&amp;rsquo;t know what you don&amp;rsquo;t know :))) ). MÌnh bắt đầu đi tìm hiểu thử về vấn đề này. Mình bắt gặp một &lt;a href=&#34;https://github.com/open-mmlab/mmediting/issues/59&#34;&gt;Github issue&lt;/a&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; và sau khi đọc nó mình đã hiểu ra vấn đề.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thật ra việc generalization của Super Resolution không có vấn đề gì cả, chỉ là mình hiểu sai nó. Với đa số cách tiếp cận hiên tại, với input chỉ gồm ảnh low resolution, models có thể generalize trên những image domain khác, nhưng không thể generalize trên những downsample kernel \(  k \) khác nhau tốt được. Và bằng việc tự mình generate low resolution frames cho Vid4 dataset, mình đã dùng một downsample kernel khác với downsample kernel mà model đã được optimized. Điều đó giải thích cho việc tại sao kết quả của mình không tốt. Ngoài ra, mình còn biết được thêm là có một nhánh nghiên cứu Super Resolution mà models có thể generalize với nhiều downsample kernels khác nhau,  gọi là Blind Super Resolution. Khi đó input cho model khi train sẽ cần thêm downsample kernel đã được dùng để sinh ra ảnh low resolution. Okay, vấn đề đã được giải quyết, mình tìm code mà đã được dùng cho dataset REDS và sử dụng nó cho dataset Vid4 và kết quả mình nhận được make sense hơn rất nhiều.&lt;/p&gt;
&lt;p&gt;Bên cạnh việc hiểu được vấn đề, sự phát hiện này cũng làm mình hơi hoang mang một tí vì bất ngờ mình có thêm một hướng nghiên cứu cần phải để tâm. Nhưng đó sẽ là một câu chuyện khác và mình sẽ kể khi mình bớt hoang mang hơn :))). Thường sau khi giải quyết xong vấn đề, mình thường dành thời gian để reflection - nhìn lại và nghĩ xem mình có thể làm tốt hơn ở những bước nào.&lt;/p&gt;
&lt;p&gt;Thật ra, trong quá trình mình giải quyết vấn đề ở trên, nếu mình bắt đầu từ bước 6 thì sẽ đỡ tốn thời gian hơn rất nhiều. Lúc mình đang phân vân giữa 2 giả thuyết overfitting và underfitting, mình cũng đã nghĩ đến generalization. Nhìn lại, mình nhận ra mình chọn 2 giả thuyết kia để thử vì việc xác thực tụi nó dễ với mình nhiều hơn là mình nghĩ chúng có thể là cốt lõi của vấn đề. Mình không tìm hiểu về generalization đầu tiên vì mình không có nhiều manh mối về nó và nó sẽ có thể làm lay chuyển những assumptions mà mình đang sử dụng trước giờ. Và đây là một bài học cho mình: trong những lựa chọn, không nên lúc nào cũng ưu tiên những lựa chọn trong &amp;ldquo;comfort zone&amp;rdquo; của mình.&lt;/p&gt;
&lt;p&gt;Ngoài ra, trong lúc tìm hiểu về Blind Super Resolution, mình tìm thấy đươc một paper có show kết quả super resolution khi mà kernel được dùng để downsample và kernel mà model học được khác nhau. Điều này giúp mình hiểu rõ hơn về kết quả thí nghiệm của mình -  điều mà trước đây luôn làm mình bối rối. Mình cũng thấy khá thắc mắc, là hầu như trong những papers mình đọc qua, không có paper nào nhắc qua về những kết quả kiểu như thế này. Thật ra mình cũng không thấy bất ngờ lắm, vì mình nghĩ hầu như các papers cũng sẽ đều tập trung vào kết quả tốt và bỏ đi những kết quả &amp;ldquo;thất bại&amp;rdquo;. Mình cũng từng đọc qua điều này trong quyển &lt;a href=&#34;https://www.goodreads.com/book/show/18693884-how-not-to-be-wrong&#34;&gt;How Not to Be Wrong: The Power of Mathematical Thinking&lt;/a&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, trong đó tác giả đề xuất rằng những thí nghiệm thất bại nên được coi trọng hơn và được đưa vào trong những papers nhiều hơn. Mình nghĩ lúc mình viết thesis, mình cũng sẽ cố đưa những kết quả thí nghiệm dù thất bại nhưng vẫn có giá trị vào :)).&lt;/p&gt;
&lt;p&gt;Vừa rồi là những suy nghĩ linh tinh của mình. Hẹn gặp lại các bạn trong những bài viết tiếp theo  \ (•◡•) /.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Các bạn có thể tìm đọc tất cả bài viết của series này tại &lt;a href=&#34;https://incenger.github.io/categories/fellowship-optim/&#34;&gt;đây&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://incenger.github.io/post/fellowship_optim/random_things_2/&#34;&gt;https://incenger.github.io/post/fellowship_optim/random_things_2/&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.manning.com/books/human-in-the-loop-machine-learning&#34;&gt;https://www.manning.com/books/human-in-the-loop-machine-learning&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmediting/issues/59&#34;&gt;https://github.com/open-mmlab/mmediting/issues/59&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.goodreads.com/book/show/18693884-how-not-to-be-wrong&#34;&gt;https://www.goodreads.com/book/show/18693884-how-not-to-be-wrong&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
        
      </description>
    </item>
    
    <item>
      <title>The Fellowship of the Optim: Random things</title>
      <link>https://incenger.github.io/post/fellowship_optim/random_things_2/</link>
      <pubDate>Fri, 23 Apr 2021 19:20:42 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/fellowship_optim/random_things_2/</guid>
      <description>
        
          &lt;p&gt;Khác với những bài viết trước, bài viết này sẽ có thể hơi lộn xộn vì đó cũng chính là tâm trạng của mình khi gõ những dòng này.
Mình đã định không viết gì vào tuần này. Một phần vì mình không tìm được nguồn cảm hứng để viết (những bài viết trước mình luôn viết trong lúc cảm hứng rất dồi dào) và cũng vì mình cũng chưa nghĩ ra được một câu chuyện hoàn chỉnh và hay ho để kể cho mọi người.
Nhưng mà, mình từng bảo mình sẽ viết dù không có ai đọc mà, vậy thì tại sao mình phải áp lực chính bản thân mình về nội dung của nó nhỉ? Cứ viết thôi :))).&lt;/p&gt;
&lt;p&gt;Tuần này có những câu chuyện rời rạc, nhưng suy nghĩ còn dở dang của mình&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;fail-silently&#34;&gt;Fail silently&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;Neural network fails silently&amp;rdquo;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;  - mình đã trải nghiệm điều này khá nhiều lần, và tuần này mình vừa được trải nghiệm nó thêm một lần nữa =)))&lt;/p&gt;
&lt;p&gt;Trong quá trình phát triển segmentation model, mình có viết một đoạn code nhỏ để tính pixel-level accuracy giữa predicted mask và groundtruth mask. Thụât toán thì cũng không có gì phức tạp, chỉ cần đọc vào ảnh và groundtruth tương ứng, chuyển chúng thành numpy array và so sánh pixel-wise là được. Vì nó đơn giản nên mình cũng không nghĩ là nó sẽ có bug :((&lt;/p&gt;
&lt;p&gt;Đoạn code mình tính average accuracy như thế này:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;avg_acc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;average&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seg_mask&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Khi chạy code thì mình nhận được một warning như thế này&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DeprecationWarning: elementwise comparison failed; this will raise an error in the future.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Vì nó là &lt;code&gt;DeprecationWarning&lt;/code&gt; và kết quả average trên tất cả các ảnh cũng ra một con số có nghĩa nên là mình cũng không để tâm lắm. Mình có để ý là kết quả evaluation hơi thấp hơn mình mong đợi. Mình nghĩ là chắc do đây là  baseline model, và mình đã dành thời gian để cải thiện phần model training với mục đích tăng kết quả evaluation lên. Sau khi mình implement một số kĩ thuật training tốt hơn, kết quả evaluation có tăng nhưng không đáng kể và nó vẫn thấp hơn mình mong đợi. Mình bắt đầu cảm thấy bối rối (mình nghĩ với ai làm về Deep Learning thì đều đã trải qua cảm giác này). Và những lúc bối rối như thế này, mình bắt đầu kiểm tra lại mọi thứ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data: Data mình lấy trực tiếp từ người khác, nên khả năng có lỗi cũng không cao, mà nếu có lỗi gì đó thì thường sẽ xảy ra lúc training mà training ổn nên tạm coi như data đã ổn.&lt;/li&gt;
&lt;li&gt;Prediction của model: Visualize predicted segmentation mask thì mình vẫn thấy không có vấn đề gì.&lt;/li&gt;
&lt;li&gt;Kiểm tra colorspace của predicted mask và groundtruth mask đã giống nhau chưa. Cả hai đã giống nhau, đều là grayscale image.&lt;/li&gt;
&lt;li&gt;Ơ thế thì không lẽ model của mình có vấn đề &amp;hellip;., cái này là ít khả năng nhất, và nếu debug thì sẽ tốn rất nhiều thời gian :(((&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tới đây mình bắt đầu cảm thấy hơi hoang mang, và trong những lúc như vậy mình quyết định làm mọi thứ cực kì cẩn thận. Mình nhớ đến cái warning khi này, mặc dù biết là nó cũng không liên quan gì nhưng mình quyết định cứ refactor lại nó (vì mình cũng chưa biết làm gì tiếp). Thế là mình search document của numpy và viết lại nó thành như thế này&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;avg_acc&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;average&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;equal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seg_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gt_mask&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Mình chạy lại code để đảm bảo cái warning đó đã mất đi. Đúng là nó mất đi thật, nhưng code của mình lại báo lỗi (•_•) (refactor xong xuất hiện thêm lỗi là có thật).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;ne&#34;&gt;ValueError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;operands&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;could&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;broadcast&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;together&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shapes&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;433&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;577&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;480&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;640&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ồ&amp;hellip; hóa ra là vậy :(((. Data mình có vấn đề - size của input image và groundtruth mask bị sai lệch vài pixel. Và với 2 numpy array có shape khác nhau, khi thực hiện element-wise comparision như trên thì nó sẽ không báo lỗi mà trả về giá trị &lt;code&gt;False&lt;/code&gt;, và khi pass &lt;code&gt;False&lt;/code&gt; vào &lt;code&gt;np.average()&lt;/code&gt; thì giá trị nhận được sẽ là &lt;code&gt;0.0&lt;/code&gt;. I&amp;rsquo;m fine ಥ_ಥ.&lt;/p&gt;
&lt;h3 id=&#34;im-stuck&#34;&gt;I&amp;rsquo;m stuck&lt;/h3&gt;
&lt;p&gt;Mình hay làm những task của thesis (do mình đặt ra) vào buổi tối. Nhưng thường sau khi dành cả ngày để tập trung vào công việc của công ty, mình thường cảm thấy hơi cạn kiệt năng lượng và cảm hứng. Và khi phải đối mặt với những task cần sự tập trung và nhiều năng lượng như implement một model, code của mình nó sẽ trở nên như thế này &amp;hellip;. Mặc dù biết là không tốt, nhưng lúc đó mình chỉ cần nó train được và losses trả về hợp lí là đươc :&#39;(.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/random_things_2/dumb_block.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/random_things_2/dumb_net.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Mình đang bị kẹt trong việc phát triển model của mình. Thật ra đây không phải là lần đầu mình rơi vào tình trạng này. Đó là trạng thái sau khi bạn đã phát triển thành công một baseline model, thử một số phương pháp phổ biến nhất để cải thiện nó và đạt được một kết quả tạm chấp nhận được nhưng không phải là cái mà bạn đang mong đợi. Lúc đó, bạn sẽ phải trả lời câu hỏi &amp;ldquo;What&amp;rsquo;s next?&amp;rdquo;. Đúng vậy, làm gì tiếp theo đây? Trước đây, mình thường hay giải quyết bằng cách thực hiện những thay đổi nhỏ trong hyperparameters, sau đó train lại và cầu nguyện cho nó hiệu quả. Phương pháp đó làm mình cảm thấy mình productive, vì mình vẫn đang làm gì đó, nhưng thật sự thì nó không hiệu quả lắm khi mà nếu model của bạn đã ổn định thì hầu như những thay đổi đó cũng không có ích gì mấy. Hiện tại mình đã đưa việc &amp;ldquo;productive ảo&amp;rdquo; này lên một tầm cao mới: thay vì chỉ thực hiện hyper-parameter tuning, mình suy nghĩ về việc tạo ra model khác bằng việc lắp ghép những ý tưởng khác nhau lại và implement tụi nó =)))). (Mình cảm thấy mình khá giỏi trong việc tự đánh lừa chính bản thân mình =))) ).&lt;/p&gt;
&lt;p&gt;Việc productive một cách không thật sự productive như vậy càng làm mình cạn kiệt năng lượng. Mình nghĩ mình sẽ không lao đầu vào cái này nữa (rút kinh nghiệm từ những lần trước =))) ). Dự định của mình là &amp;ldquo;đi chậm&amp;rdquo; lại một tí, tạm gác những model qua một bên và tìm hiểu kĩ hơn về vấn đề mình cần giải quyết. Mình cần phải chấp nhận là mình sẽ tiến độ của mình sẽ chậm đi, nhưng mình hi vọng sau đó mình sẽ hiểu rõ hơn về những thí nghiệm mà mình muốn thử, thay vì làm một cách hơi &amp;ldquo;mù quáng&amp;rdquo; như bây giờ. Mình cảm thấy ngoài việc có progress, thì việc giữ được tinh thần bản thân ở một mức ổn cũng là việc thật sự cần thiết. Hôm nay mình đọc được một câu cũng khá là hay (trong một bài viết cũng khá là hay nốt), tiếp thêm cho mình một tí động lực để tạm dừng việc thử nghiệm ý tưởng một cách vô tội vạ này :)))&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All the very good researchers I know try lots of ideas. (Pure volume isn’t enough, they try their ideas with purpose, but they try a lot of ideas with purpose.)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;this-makes-my-day&#34;&gt;This makes my day&lt;/h3&gt;
&lt;p&gt;Trong những ngày tâm trạng không ổn lắm như thế này, mình vô tình tìm được một thread Reddit rất hay mà mình tìm kiếm bấy lâu nay. Mình hay tìm kiếm những newsletter và blog liên quan đến AI, Deep Learning, và Machine Learning để có thể cập những kiến thức và công nghệ mới. Hiện tại thì mình đã có vài trang, nhưng cảm thấy vẫn không ổn lắm. Và khi mình đang nằm trên giường và lướt qua thread này, mình phải bật dậy và lập tức bookmark lại từng mục trong đó ngay lập tức :))). Đây là một thread tổng hợp lại những blogs, newsletter, youtube channels, paper summary, &amp;hellip; từ những researchers và các tổ chức hàng đầu về AI. Nếu các bạn cũng đang cần tìm thì có thể xem &lt;a href=&#34;https://www.reddit.com/r/MachineLearning/comments/mwwftu/d_your_favorite_ai_podcasts_blogs_newsletters/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&#34;&gt;ở đây&lt;/a&gt; nhé.&lt;/p&gt;
&lt;p&gt;Tuần này như vậy thôi, hẹn gặp lại các bạn vào những bài viết tiếp theo \ (•◡•) /.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Các bạn có thể tìm đọc tất cả bài viết của series này tại &lt;a href=&#34;https://incenger.github.io/categories/fellowship-optim/&#34;&gt;đây&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://karpathy.github.io/2019/04/25/recipe/&#34;&gt;http://karpathy.github.io/2019/04/25/recipe/&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.alexirpan.com/2021/04/07/grad-school-5years.html&#34;&gt;https://www.alexirpan.com/2021/04/07/grad-school-5years.html&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;

        
      </description>
    </item>
    
    <item>
      <title>The Fellowship of the Optim: How I met my thesis</title>
      <link>https://incenger.github.io/post/fellowship_optim/where_am_i_now_1/</link>
      <pubDate>Sat, 17 Apr 2021 19:14:34 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/fellowship_optim/where_am_i_now_1/</guid>
      <description>
        
          &lt;p&gt;Nếu như đã đọc &lt;a href=&#34;https://incenger.github.io/post/fellowship_optim/hello_world_0/&#34;&gt;bài viết trước&lt;/a&gt;, các bạn sẽ biết mình đang làm thesis về đề tài Video Super Resolution. Thật ra đề tài này không phải là dự định ban đầu của mình, đúng hơn, mình còn chưa bao giờ nghĩ đến đề tài này cho đến khi gặp thầy hướng dẫn. Trong bài viết này, mình sẽ kể lại mình đã bắt đầu với thesis của mình như thế nào, và tình hình hiện tại của nó.&lt;/p&gt;
&lt;h3 id=&#34;mình-lên-ý-tưởng-không-hẳn-cho-thesis-của-mình-như-thế-nào&#34;&gt;Mình lên ý tưởng (không hẳn) cho thesis của mình như thế nào?&lt;/h3&gt;
&lt;p&gt;Mình khá hoang mang với việc làm thesis, và việc nghĩ đến nó thôi cũng làm mình lo lắng. Mặc dù mình đã có một ít kinh nghiệm về việc làm nghiên cứu, nhưng thẳng thắn mà nói thì mình chưa có một &amp;ldquo;công trình&amp;rdquo; hoàn chỉnh nào cả. Dự án ở môn Nghiên cứu khoa học thì không có kết quả đáng kể nào cả, còn dự án mình làm ở AI Lab thì vẫn chưa hoàn thành lúc mình quyết định xin nghỉ. Lúc đó, mình không định hình được mình sẽ cần phải làm gì có thể tạo nên một công trình nghiên cứu thành công cả. À thật ra về mặt lý thuyết thì mình biết, nhưng mình không biết phải làm mọi thứ như thế nào. Mình biết làm thesis chắc chắn sẽ là không phải là một con đường dễ dàng cho mình, nhưng trong những lựa chọn mình có, thì nó vẫn là lựa chọn mà mình cảm thấy tự tin nhất. Challenge accepted, bắt tay vào làm thôi.&lt;/p&gt;
&lt;p&gt;Những việc đầu tiên mình cần làm đó là tìm đề tài, tìm người làm thesis chung, và tìm người hướng dẫn. Lúc lên ý tưởng cho thesis, mình vẫn còn đang làm ở CinnamonAI và tập trung vào những bài toán liên quan đến Graph Neural Network (mình sẽ gọi tắt là GNN). Với việc đang là một chủ đề khá mới và hấp dẫn, mình nghĩ làm thesis về Graph Neural Network sẽ là một lựa chọn hợp lí. Nhưng tìm kiếm bài toán vốn không phải là điểm mạnh của mình. &amp;ldquo;Làm về Graph Neural Network, nhưng làm gì bây giờ?&amp;rdquo; - đây là câu hỏi mà mình luôn trăn trở trong suốt một thời gian dài. Mình suy nghĩ về một số hướng mình có thể làm, nhưng cảm thấy đều bế tắc cả. Đề xuất một network architecture mới -  mình chỉ nghĩ được đến việc áp dụng những architecture đã hoạt động hiệu quả trong những bài toán về Vision hay Natural Lanuage vào Graph, nhưng hầu như những sự kết hợp đó đều được được thực hiện. Mình cũng nhận thấy một vấn đề nho nhỏ là những dataset được dùng trong các papers về GNN rất là nhỏ, đặc biệt so với kích thước của model, nên mình nghĩ tìm cách đưa ra một dataset mới (kiểu như ImageNet) sẽ là một đóng góp đáng kể. Nhưng mình nghĩ, đó không phải là một hướng đi khả thi đối với mình. Mãi một thời gian sau, mình cũng vẫn loay hoay trong những suy nghĩ rối bời này. Lúc đó, nếu ai hỏi, thì mình cũng sẽ luôn chỉ trả lời là mình sẽ làm về GNN và không nói thêm được gì nữa.&lt;/p&gt;
&lt;p&gt;Vì không muốn áp lực lên bản thân quá nhiều, cộng với việc mình vẫn phải tập trung vào một số môn học ở trường, mình thường xuyên trì hoãn về việc tìm kiếm ý tưởng cho thesis. Lúc này, mình quyết định chuyển hướng qua việc đi tìm bạn làm chung vì dù sao hai người động não suy nghĩ thường sẽ tốt hơn một người, nhất là một người đang bế tắc :))). Mình bắt đầu đi rủ những đứa bạn mà mình nghĩ sẽ có thể hứng thú về Deep Learning để làm chung thesis. Và sau đó, sao nhỉ, là một chuỗi những lời từ chối khi những người mình hỏi nếu không phải đã quyết định làm một mình, thì cũng đã có nhóm sẵn rồi. Lúc đó, mình cảm giác như mình đang là một diễn viên hài trong một bộ phim và khán giả đang cười vào tình huống trớ trêu này của mình.&lt;/p&gt;
&lt;p&gt;Mình tạm gác hai việc tìm người làm chung và tìm ý tưởng qua một bên, và bắt đầu đi liên hệ với những thầy cô mình biết để tìm giáo viên hướng dẫn. Mình lên hệ thầy Triết, nói về việc mình đang dự định làm về Graph Neural Network, và hỏi thầy có biết những thầy cô nào có thể giúp mình trong đề tài này không. Vì mình cũng chưa có bài toán cụ thể, nên thầy cũng không giúp gì được cho mình nhiều. Thầy khuyên mình cứ suy nghĩ thêm, khi nào có hướng đi cụ thể thì thầy sẽ giúp được và động viên mình. Mình không biết thầy có cảm nhận được tinh thần đang chạm đáy của mình lúc đó qua những tin nhắn bình thường hay không, nhưng những lời động viên của thầy giống như những tia sáng lé loi duy nhất mà mình cảm nhận được trong suốt khoảng thời gian đó. Và đúng là sau những lời động viên thì mọi chuyện của mình bắt đầu trở lại đúng quỹ đạo thật.&lt;/p&gt;
&lt;p&gt;Một đứa bạn khác của mình (mà mình không nghĩ là sẽ làm thesis về Deep Learning) có nhắn hỏi về việc mình đã có nhóm chung với ai chưa. Và thế là mình đã tìm được người làm chung theo cái cách mà mình không ngờ nhất =))). Tiếp theo, mình liên hệ với thầy Sơn - người dạy mình 2 môn Computer Graphics và Computer Vision - để hỏi về việc hướng dẫn luận văn. Sau khi nghe mình chia sẻ về việc đang cảm thấy bế tắc trong việc tìm kiếm đề tài, thầy đã đề xuất cho mình một số đề tài. Sau một thời gian suy nghĩ và bàn bạc với bạn mình, mình đã quyết định chọn chủ đề về Video Super Resolution (một trong những chủ đề mà thầy mình đề xuất). Và thế là đề tài của mình ra đời. Nhìn lại, có lẽ vấn đề của mình không phải là việc tìm ra bài toán, vì lúc mình chọn Video Super Resolution mình cũng đã nghĩ ra được hướng đi nào đâu, mà là tìm một người có thể dẫn dắt mình, giúp mình có thể tự tin với lựa chọn dù chưa biết phía trước điều gì đang chờ đón.&lt;/p&gt;
&lt;h3 id=&#34;mình-bắt-đầu-như-thế-nào-&#34;&gt;Mình bắt đầu như thế nào ?&lt;/h3&gt;
&lt;p&gt;Vừa rồi là phần hồi tưởng của mình về quá khứ, phần tiếp theo sẽ về hiện tai và mình nghĩ sẽ một số điều thú vị mà các bạn có thể thu về được cho mình. (thật ra nó cũng là quá khứ nhưng vẫn tiếp tục trong hiện tại - kiểu như thì hiện tại hoàn thành trong tiếng Anh vậy :)) ). Mình hiểu Super Resolution là làm gì (mình nghĩ đa phần mọi người cũng sẽ hiểu) nhưng mình chưa tìm hiểu sâu về đề tài này. Trong những course Deep Learning mà mình học, mình nhớ cũng không có course nào sử dụng bài toán này làm ví dụ (nếu các bạn biết có course nào thì có thể comment để mình cập nhật lại nhé, vì lâu rồi mình cũng không tìm những course Deep Learning mới). Vì vậy, mình quyết định bằng cách đơn giản nhất là &amp;hellip; đọc paper :)). Và đây là cách mình tìm kiếm papers và những công cụ mình sử dụng kèm theo để tổng hợp kiến thức.&lt;/p&gt;
&lt;p&gt;Để tìm papers thì mình sẽ sử dụng &lt;a href=&#34;https://scholar.google.com/&#34;&gt;Google Scholar&lt;/a&gt;,  &lt;a href=&#34;https://www.connectedpapers.com/&#34;&gt;Connected Papers&lt;/a&gt;  và &lt;a href=&#34;https://paperswithcode.com/&#34;&gt;Paperswithcode&lt;/a&gt;. Google Scholar để mình có thể tìm paper chính xác hơn, thay vì chỉ search bằng Google thông thường. Connected Papers để tìm những papers liên quan đến một papers nào đó và Paperswithcode giúp mình tìm những papers đang là &amp;ldquo;state-of-the-art&amp;rdquo; và nhanh chóng biết được papers đó có code sẵn không. Mình sẽ lấy một ví dụ cho các bạn dễ hiểu.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Đầu tiên, mình sẽ thử search Google Scholar với từ khóa &amp;ldquo;Video Super Resolution&amp;rdquo;. Mình thường sẽ chọn những papers gần đây hoặc có số citation cao để bắt đầu. Mình chọn paper &lt;strong&gt;Video super-resolution with convolutional neural networks&lt;/strong&gt;.
&lt;img src=&#34;https://incenger.github.io/img/2021/how_i_met_my_thesis_1/google_scholar_search.png&#34; alt=&#34;Google Scholar&#34;&gt;&lt;/li&gt;
&lt;li&gt;Tiếp theo, mình có thể sử dụng Connected Papers để tìm những papers liên quan đến papers này.
&lt;img src=&#34;https://incenger.github.io/img/2021/how_i_met_my_thesis_1/connected_papers.png&#34; alt=&#34;Connected Papers&#34;&gt;&lt;/li&gt;
&lt;li&gt;Mình sẽ dùng thêm Paperswithcode để tìm, qua đó sẽ biết thêm được những dataset đang được sử dụng cho bài toán này và các phương pháp hiện nay đang có kết quả như thế nào.
&lt;img src=&#34;https://incenger.github.io/img/2021/how_i_met_my_thesis_1/paperswithcode.png&#34; alt=&#34;Papers With Code&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sau khi tìm xong thì mình bắt đầu đọc paper thôi. Về cách đọc paper thì đã có rất nhiều hướng dẫn chi tiết và cách mình đọc paper thì cũng không có gì khác biệt lắm. Mình thường đọc và sử dụng Obsidian.md để take notes lại. Mình thường take note theo 4 câu hỏi chính: &amp;ldquo;Bài toán ở đây là gì?&amp;rdquo;, &amp;ldquo;Phương án giải quyết nó như thế nào?&amp;rdquo;, &amp;ldquo;Kết quả ra sao?&amp;rdquo;, &amp;ldquo;Mình có thể tái sử dụng hay mở rộng ý tưởng ở đây như thế nào?&amp;rdquo;. Mình khá thích Obsidian.md là mình có thể link những note lại với nhau và tạo thành một cái knowledge graph. Công cụ này giúp mình kết nối những papers lại với nhau dễ dàng hơn một tí (vẫn tốn công sức ban đầu, nhưng sau này nếu các bạn có xem lại thì sẽ dễ có được góc nhìn tổng quan hơn).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/how_i_met_my_thesis_1/obsidian_note.png&#34; alt=&#34;Ví dụ về một paper note&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/how_i_met_my_thesis_1/obsidian_graph.png&#34; alt=&#34;Tính năng Graph của Obsidian.md&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ngoài phần papers ra thì một phần quan trọng không kém đó là code. Trong lúc tìm paper, mình cũng thường ưu tiên những papers có code (chính chủ hoặc re-implementation đều được, bạn có thể dùng Paperswithcode để tìm). Mình sẽ cần phải implement và thử nghiệm khá nhiều phương pháp, nên mình cần những codebase thuận tiện cho việc mở rộng. Thuận tiện ở đây mang ý nghĩa: mình cần có tốn nhiều công sức để thêm một architecture hoàn toàn mới không; những utility functions như training, evaluation có tái sử dụng được không, &amp;hellip; (Vì mình lười nên tự implement luôn là phương án cuối cùng của mình :)) ). Cuối cùng, mình quyết định lựa chọn &lt;a href=&#34;https://github.com/open-mmlab/mmediting&#34;&gt;MMEditing&lt;/a&gt; vì nó đáp ứng được khá nhiều tiêu chí ở trên, thiết kế khá đơn giản, và cũng đang được maintained. Mình không giỏi về code lắm, nên mình dành khá nhiều thời gian để tìm hiểu và học về repo này. Cách mình học thì cũng không có gì đặc biệt lắm, cứ chạy code, chèn cái snippet này vào &lt;code&gt;__import__(&#39;ipdb&#39;).set_trace()&lt;/code&gt; và chạy từng dòng để hiểu mọi thứ hoạt động như thế nào =)))) (Nó hiệu quả hơn &lt;code&gt;print(&amp;quot;What the heck?&amp;quot;)&lt;/code&gt; nhiều đấy, các bạn cứ thử xem :)) ).&lt;/p&gt;
&lt;h3 id=&#34;thesis-đến-đâu-rồi--&#34;&gt;Thesis đến đâu rồi :&#39;( ?&lt;/h3&gt;
&lt;p&gt;Trong giai đoạn này (sau khi kết thúc kì 1 năm 4, khoảng từ tháng 1/2021) mình cũng khá là trì hoãn trong việc làm thesis. Sau đây là một số lí do biện hộ cho việc mình trì hoãn =))):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mình làm full-time nên ban ngày mình sẽ tập trung hoàn toàn vào những bài toán ở công ty, thời gian còn lại thì mình muốn nghỉ ngơi. Ngoài ra, thỉnh thoảng mình cũng thích làm công việc của công ty hơn là làm thesis :))&lt;/li&gt;
&lt;li&gt;Mình cần hoàn thành thesis proposal. Do vậy mình dành thời gian chủ yếu để đọc papers và trì hoãn việc code lại. Hậu quả là sau khi hoàn thành proposal thì mình chưa thật sự chạy thử một model nào cả.&lt;/li&gt;
&lt;li&gt;Codebase &lt;code&gt;MMEditing&lt;/code&gt; khá lớn, mình không biết nên bắt đầu từ đâu.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Trong thời gian này mình còn nhận ra được một bài học khá thú vị. Mình đọc ở đâu đó lời khuyên là bạn nên làm 2 projects cùng một lúc, để nếu có một cái bế tắc thì bạn sẽ cảm thấy mình productive khi làm cái còn lại. Mình thấy lời khuyên này khá đúng, nhưng chỉ là, mình lại gặp bế tắc ở cả 2 projects (công ty và thesis) cùng một lúc. Kết cục là mình cảm giác như một thằng phế vậy =)) (Nếu các bạn muốn nghe về chuyện projects ở công ty thì cứ comment nhé, mình sẽ kể ở một bài khác, nó cũng là một bài học đau thương cho mình). Đây là biểu đồ tâm trạng mỗi ngày của mình trong thời gian đó (màu xanh lá cây là ổn, màu xanh nhạt là không ổn, màu cam là tệ).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://incenger.github.io/img/2021/how_i_met_my_thesis_1/mood.jpg&#34; alt=&#34;Thống kê cảm xúc của mình trong tháng 3/2021. Mình dùng app Daylio&#34;&gt;&lt;/p&gt;
&lt;p&gt;Một bài học nữa (mình nghĩ chắc các bạn cũng nghe nhiều rồi) mà mình rút ra được là không nên để những nỗi sợ ban đầu làm bạn chùn bước. Mình đã lo sợ trước thesis proposal, cảm thấy hoang mang giữa một codebase mà mình nghĩ chắc mình sẽ không hiểu được. Nhưng từng bước một, mình dần dần vượt qua được những chướng ngại đó. Đây là bài học của một người bình thường (mình nghĩ đúng hơn là hơi &amp;ldquo;phế&amp;rdquo;) nên mình nghĩ có thể nó sẽ dễ đồng cảm với các bạn hơn một người thành công :)).&lt;/p&gt;
&lt;p&gt;Tóm lại, về thesis, hiện tại thì mình đã hoàn thành xong thesis proposal, đã hiểu code &lt;code&gt;MMEditing&lt;/code&gt;, implement xong một baseline model và đang train để xem kết quả như thế nào =))). Đây là giai đoạn mình thích nhất: được thử nghiệm nhiều ý tưởng và cầu nguyện mỗi lần train model =))))&lt;/p&gt;
&lt;p&gt;Bài viết hôm nay sẽ hơi dài một tí, vì mình dự định sẽ viết mỗi tuần 1 bài (chắc là vào cuối tuần). Nếu mình rảnh thì mình có thể viết nhiều hơn, nhưng mình không hứa chắc được, phải làm mới có thứ mà chia sẻ chứ :))). Những bài viết sau mình sẽ viết cụ thể mình đang làm gì, vấn đề mình gặp phải và mình giải quyết nó như thế nào. Đưới đây là một ví dụ (mà mình mới giải quyết xong hôm nay).&lt;/p&gt;
&lt;p&gt;Khi mình evalute kết quả Super Resolution bằng việc sử dụng thuật toán cực kì đơn giản là resize bằng bicubic interpolation, kết quả PSNR (Peak Signal to Noise Ratio) của mình khá là cao, khoảng 31 db, trong khi số liệu trong các papers cũng chỉ ở khoảng 26 db. Mình bắt đầu kiểm tra lại mọi thứ:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code evaluation đã chính xác. Mình dùng hàm có sẵn trong &lt;code&gt;MMEditing&lt;/code&gt; nên xác suất sai là khá thấp. Mình kiểm tra thêm trong phần issues của repo thử xem có cái nào liên quan đến PSNR luôn không cho chắc.&lt;/li&gt;
&lt;li&gt;Mình evaluate thêm kết quả của model hiện tại mình đang train, ra được PSNR khoảng 33 db, cao hơn khá nhiều so với model trong một số papers trong khi model của mình cũng chỉ là baseline. Lúc này mình bắt đầu nghĩ là có thể nằm ở data.&lt;/li&gt;
&lt;li&gt;Mình visualize kết quả High Resolution và thấy kết quả cũng không quá tốt, và đạt PSNR cao như vậy thì rõ ràng là có vấn đề.&lt;/li&gt;
&lt;li&gt;Mình kiểm tra thêm metric khác là SSIM (Structural Similarity Index Measure) và thấy kết quả của metrics này khá tương đồng với các papers, điều này có nghĩa là cách mình dùng PSNR có vấn đề.&lt;/li&gt;
&lt;li&gt;Mình nhớ lại là một số papers evaluate PSNR trên Y channel trong định dạng YCbCr thay vì RGB như thông thường. Mình implement hàm để covert ảnh sang YCbCr và thử lại. Kết quả ra đúng như mình mong đợi -&amp;gt; Problem solve :))).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;À còn một điều nữa mà mình cần làm, là tìm hiểu về định dạng màu YCbCr và tại sao PSNR lại không hoạt động tốt với định dạng RGB. Mình sẽ chia sẻ sau khi mình hiểu nhé :&amp;quot;&amp;gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Các bạn có thể tìm đọc tất cả bài viết của series này tại &lt;a href=&#34;https://incenger.github.io/categories/fellowship-optim/&#34;&gt;đây&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;

        
      </description>
    </item>
    
    <item>
      <title>The Fellowship of the Optim: Hello World</title>
      <link>https://incenger.github.io/post/fellowship_optim/hello_world_0/</link>
      <pubDate>Wed, 14 Apr 2021 23:42:14 +0700</pubDate>
      
      <guid>https://incenger.github.io/post/fellowship_optim/hello_world_0/</guid>
      <description>
        
          &lt;p&gt;Mình sẽ bắt đầu bằng cách cung cấp một số thông tin cơ bản nhất, để bạn có thể biết được mình là ai, và những bài viết này nhằm mục đích gì. Và cách bắt đầu hiệu quả nhất theo mình nghĩ là những câu hỏi.&lt;/p&gt;
&lt;h3 id=&#34;tại-sao-lại-là-the-fellowship-of-the-optim&#34;&gt;Tại sao lại là The Fellowship of the Optim?&lt;/h3&gt;
&lt;p&gt;Mình lấy ý tưởng từ The Lord of the Rings - một trong những tác phẩm mình rất yêu thích. Mình thấy một phần nào đó của mình trong Frodo, khi vốn là một người vô năng nhưng lại được giao cho trọng trách lớn nhất. Giống như cách Frodo luôn tự vấn về bản thân, mình cũng hay nghĩ tại sao mình lại có được những gì mình có hiên tại. Ngoài ra, mình thấy cuộc hành trình tiêu hủy chiếc nhẫn, chông gai và đầy cạm bẫy, giống như hành trình một Optimizer đi tìm đến một điểm cực trị thích hợp vậy. Và mình cũng thích từ Fellowship, vì mình mong muốn tìm được những người đồng hành với mình trong những cuộc hành trình tiếp theo.&lt;/p&gt;
&lt;h3 id=&#34;tại-sao-mình-lại-có-ý-tưởng-viết-những-bài-viết-này&#34;&gt;Tại sao mình lại có ý tưởng viết những bài viết này?&lt;/h3&gt;
&lt;p&gt;Mình nghĩ mọi người thỉnh thoảng sẽ có cảm giác giống mình, là cảm giác nửa năm hay một năm trôi qua quá nhanh và khi nhìn lại thì nhận ra mình chưa làm được gì nhiều. Một cách để làm cảm giác đó trở nên ít tồi tệ đi là thêm vào dòng thời gian của mình những cột mốc cụ thể. Những bài viết này đối với mình chính là những cột mốc cho một giai đoạn đặc biệt mà mình nghĩ mình sẽ chỉ được trải nghiệm một lần trong đời&lt;/p&gt;
&lt;h3 id=&#34;mình-là-ai-mình-đang-làm-gì-và-mình-sẽ-viết-những-gì&#34;&gt;Mình là ai, mình đang làm gì, và mình sẽ viết những gì?&lt;/h3&gt;
&lt;p&gt;Các bạn có thể tìm đươc thông tin của mình thông qua những mạng xã hội đính kèm ở blog này. Vì blog này sẽ liên quan đến Deep Learning, nên mình sẽ chia sẻ một chút về hành trình của mình với Deep Learning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mình bắt đầu chú ý đến Deep Learning vào khoảng giữa kì 2 của năm thứ nhất (2018), mình cũng bắt đầu bằng những khóa học trên Coursera giống mọi người. Nhưng lúc đó, mình cứ mãi theo đuổi việc hoàn thành những khóa học mà bỏ quên việc hiểu sâu vào bản chất của Machine Learning và Deep Learning, và điều đó để lại hậu quả xấu sau này.&lt;/li&gt;
&lt;li&gt;Đầu năm 2 đại học, mình có cơ hội làm một project Deep Learning đầu tiên trong môn Nghiên cứu khoa học. Đó là bài toán về phát hiện xe bất thường trên cao tốc của NVIDIA AI City Challenge. Lúc đó mình rất phấn khởi, nhưng ngay lập tức sau đó ăn hành vì bị choáng ngợp trước việc cài đặt CUDA, cài đặt thư viện, chạy model,&amp;hellip;. Do lúc học những khóa học Deep Learning, mình chỉ tập trung vào việc hoàn thành bài tập mà không tìm hiểu sâu về bài toán, nên khi làm project này, sau khi clone được code của một paper có sẵn, thêm thắt chút đỉnh, mình không biết làm gì thêm nữa. Và sau khi kết thúc dự án này, mình cảm thấy chán Deep Learning, khi mình đã dành rất nhiều thời gian cho nó, nhưng lại luôn có cảm giác mình chẳng thật sự hiểu gì&lt;/li&gt;
&lt;li&gt;Sau dự án này, mình apply vào phòng thí nghiệm AI Lab của trường với mục đích sẽ được học hỏi và củng cố lại kiến thức của mình, và cũng thử sức với một bài toán mới lạ hơn trong lĩnh vực Speech Processing. Nhưng trái với những gì mình trông đợi, mình không nhận được nhiều sự hướng dẫn, mà chủ yếu phải tự mày mò tìm hiểu. Mình cứ dần chìm trong biển kiến thức mênh mông đó. Mình làm được trong Lab khoảng 6 tháng, và những gì mình làm được chỉ là đi thu thập dữ liệu text một cách bất cẩn, sau đó viết một vài cái rule-based (chủ yếu là regex) để làm text normalization (cho Text-to-speech system). Cuối năm 2019, mình xin nghỉ Lab, thu về cho mình thêm một dự án thất bại và bị trầm cảm. (Trong lúc này mình cũng được phỏng vấn thực tập ở Google, nhưng rồi cũng tạch :)) )&lt;/li&gt;
&lt;li&gt;Sau khi nghỉ ở Lab, mình định chuyển hướng sang học những kĩ năng Software Engineering để có thể tìm được vị trí thực tập ở năm 3. Mình vô tình thấy chương trình AI Bootcamp của CinnamonAI. Lúc đó mình đã cảm thấy quá chán nản với AI và Deep Learning, nên mình cũng không định đăng kí. Nhưng sau khi đọc qua thử chương trình, mình thấy trong bootcamp có vẻ chú trọng rất nhiều vào những kiến thức nền tảng - thứ mà mình luôn cảm thấy thiếu sót. Và mình quyết định cho AI thêm một cơ hội nữa. Và hóa ra đó là một quyết định làm thay đổi mình hoàn toàn. Mình may mắn thể hiện tốt trong Bootcamp và sau đó được offered internship và sau đó trở thành nhân viên chính thức. Có thể nói mình bắt đầu &amp;ldquo;học lại&amp;rdquo; mọi thứ về AI và Deep Learning từ Bootcamp của CinnamonAI&lt;/li&gt;
&lt;li&gt;Về quá khứ như vậy là đủ rồi, nếu các bạn có hứng thú thì mình sẽ viết thêm về những bài học mình học được từ quá trình đau thương đó. Hiện tại, về Deep Learning, mình có những kinh nghiệm nhỏ trong một số bài toán sau
&lt;ul&gt;
&lt;li&gt;Ở Bootcamp của CinnamonAI, mình làm về bài toán Automatic Colorization, nhưng chỉ trong 2 tuần nên mình cũng không tự tin là mình hiểu nhiều lắm.&lt;/li&gt;
&lt;li&gt;Trong quá trình làm việc ở CinnamonAI, mình làm chủ yếu về Deep Metric Learning và Graph Neural Network.&lt;/li&gt;
&lt;li&gt;Công việc hiện tại của mình bao gồm bài toán Detection và Segmentation.&lt;/li&gt;
&lt;li&gt;Mình đang làm luận văn về đề tài Video Super Resolution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Các bạn có thể thấy là mình thật sự không có một định hướng cụ thể nào cả, khi những bài toàn của mình thuộc khá nhiều chủ đề so với kĩ năng và kinh nghiệm hiện có của mình :))))&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Các bạn có thể tìm đọc tất cả bài viết của series này tại &lt;a href=&#34;https://incenger.github.io/categories/fellowship-optim/&#34;&gt;đây&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
