<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Tam Le ">
<meta name="description" content="Bỗng dưng mình có một suy nghĩ, là trong những bài viết này mình nên viết như thể mình đang kể một câu chuyện cho người đọc, hay là chỉ đơn thuần là viết để ghi lại cho bản thân mình thôi. Mình có suy nghĩ này vì với bài viết này, mình cảm giác mình muốn viết lại cho chính mình nhiều hơn là viết cho một người khác đọc. Nhưng ở hai câu vừa rồi thì mình đã chọn cách viết rồi mà nhỉ? Ừ, hơi mâu thuẫn thật, giống như trang giấy mà bạn sẽ thường hay thấy khi đọc sách này.
" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="/post/fellowship_optim/you_dont_know_3/" />


    <title>
        
            The Fellowship of the Optim: You don&#39;t know what you don&#39;t know :: Hi! I&#39;m Tam Le  — Hello Friend NG Theme
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.17863a81d979b637a02cd7632a4d86e9d80563ef460fd6af1a56962efcaa066b.css">



    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">
    <meta name="theme-color" content="">



<meta itemprop="name" content="The Fellowship of the Optim: You don&#39;t know what you don&#39;t know">
<meta itemprop="description" content="Bỗng dưng mình có một suy nghĩ, là trong những bài viết này mình nên viết như thể mình đang kể một câu chuyện cho người đọc, hay là chỉ đơn thuần là viết để ghi lại cho bản thân mình thôi. Mình có suy nghĩ này vì với bài viết này, mình cảm giác mình muốn viết lại cho chính mình nhiều hơn là viết cho một người khác đọc. Nhưng ở hai câu vừa rồi thì mình đã chọn cách viết rồi mà nhỉ? Ừ, hơi mâu thuẫn thật, giống như trang giấy mà bạn sẽ thường hay thấy khi đọc sách này.
"><meta itemprop="datePublished" content="2021-05-02T12:54:23+07:00" />
<meta itemprop="dateModified" content="2021-05-02T12:54:23+07:00" />
<meta itemprop="wordCount" content="2058"><meta itemprop="image" content="/"/>
<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="/"/>

<meta name="twitter:title" content="The Fellowship of the Optim: You don&#39;t know what you don&#39;t know"/>
<meta name="twitter:description" content="Bỗng dưng mình có một suy nghĩ, là trong những bài viết này mình nên viết như thể mình đang kể một câu chuyện cho người đọc, hay là chỉ đơn thuần là viết để ghi lại cho bản thân mình thôi. Mình có suy nghĩ này vì với bài viết này, mình cảm giác mình muốn viết lại cho chính mình nhiều hơn là viết cho một người khác đọc. Nhưng ở hai câu vừa rồi thì mình đã chọn cách viết rồi mà nhỉ? Ừ, hơi mâu thuẫn thật, giống như trang giấy mà bạn sẽ thường hay thấy khi đọc sách này.
"/>




    <meta property="og:title" content="The Fellowship of the Optim: You don&#39;t know what you don&#39;t know" />
<meta property="og:description" content="Bỗng dưng mình có một suy nghĩ, là trong những bài viết này mình nên viết như thể mình đang kể một câu chuyện cho người đọc, hay là chỉ đơn thuần là viết để ghi lại cho bản thân mình thôi. Mình có suy nghĩ này vì với bài viết này, mình cảm giác mình muốn viết lại cho chính mình nhiều hơn là viết cho một người khác đọc. Nhưng ở hai câu vừa rồi thì mình đã chọn cách viết rồi mà nhỉ? Ừ, hơi mâu thuẫn thật, giống như trang giấy mà bạn sẽ thường hay thấy khi đọc sách này.
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/fellowship_optim/you_dont_know_3/" /><meta property="og:image" content="/"/><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-05-02T12:54:23+07:00" />
<meta property="article:modified_time" content="2021-05-02T12:54:23+07:00" />





    <meta property="article:section" content="Fellowship-Optim" />



    <meta property="article:published_time" content="2021-05-02 12:54:23 &#43;0700 &#43;07" />









    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CM967KRZK6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-CM967KRZK6', { 'anonymize_ip': false });
}
</script>


    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">&gt;</span>
            <span class="logo__text">cd /home/incenger</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/page/aboutme">About</a></li><li><a href="/page/now">Now</a></li><li><a href="/post/">Posts</a></li><li><a href="/tags">Tags</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
                <span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
   <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
   3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
   13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
 </svg></span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="/post/fellowship_optim/you_dont_know_3/">The Fellowship of the Optim: You don&rsquo;t know what you don&rsquo;t know</a></h2>

            
            
            

            <div class="post-content">
                <p>Bỗng dưng mình có một suy nghĩ, là trong những bài viết này mình nên viết như thể mình đang kể một câu chuyện cho người đọc, hay là chỉ đơn thuần là viết để ghi lại cho bản thân mình thôi. Mình có suy nghĩ này vì với bài viết này, mình cảm giác mình muốn viết lại cho chính mình nhiều hơn là viết cho một người khác đọc. Nhưng ở hai câu vừa rồi thì mình đã chọn cách viết rồi mà nhỉ? Ừ, hơi mâu thuẫn thật, giống như trang giấy mà bạn sẽ thường hay thấy khi đọc sách này.</p>
<p><img src="/img/2021/you_dont_know_3/left_blank.jpg" alt="Is it blank?"></p>
<blockquote>
<p>You don&rsquo;t know what you don&rsquo;t know</p>
</blockquote>
<p>Mình thích câu này mặc dù nó luôn làm mình hơi rùng mình một tí.
Thừa nhận với chính bản thân mình rằng mình đang không biết, không hiểu rõ một điều gì đó là bước đầu tiên để chúng ta có thể bắt đầu quá trình tìm hiểu và học hỏi.
Mình không biết rất nhiều thứ, và điều đó có làm mình lo lắng một chút. Nhưng cảm giác đó, với mình, không đáng sợ bằng cảm giác nhận ra mình không biết là mình không biết về thứ gì đó.
Nói một cách đơn giản, mình nghĩ điều này thường xảy ra dưới 2 dạng: Bạn không hề nhận thức được gì về điều bạn không biết, hoặc là bạn nghĩ rằng mình đã hiểu rõ về thứ đó, nhưng thật ra góc nhìn của bạn còn đang rất hạn hẹp và thiếu sót.
Ở dạng thứ nhất, thông thường bạn sẽ nhận ra nó với một cảm giác ngạc nhiên: &ldquo;Ồ, hóa ra những thứ thế này có tồn tại&rdquo;.
Ở dạng thứ hai, bạn sẽ nhận ra nó với cảm giác rùng mình: &ldquo;Vậy hóa ra lâu nay mình đang hiểu sai sao?&rdquo;
Mình không thích dạng thứ hai, nhưng mình phải thừa nhận nó cho mình nhiều bài học hơn. Nó giống như là một lời cảnh tỉnh, để mình có thể nhìn lại tất cả những &ldquo;nền móng&rdquo; mà mình đang đứng trên, liệu nó đang kiên cố và vững chãi, hay đang sẵn sàng sụp đổ bất cứ lúc nào?</p>
<p><img src="/img/2021/you_dont_know_3/dishes_falling.jpg" alt="Thế này có được coi là vững chãi không?"></p>
<p>Mình viết về câu nói này, vì gần đây mình vừa trải nghiệm nó. Để giải thích cho vấn đề này, thì mình sẽ phải nói về đề tài luận văn mình đang làm một tí. Mình làm về Video Super Resolution, nhưng để cho đơn giản, mình sẽ coi như là Single Image Super Resolution. Một khi đã hiểu được vấn đề thì các bạn có thể dễ dàng tổng quát hóa nó lên thôi.</p>
<p>Super Resolution có thể hiểu đơn giản là phóng to ảnh, ví dụ phóng to một bức ảnh có độ phân giải 640x360 (360p) thành 1280x720 (720p) (độ phóng to - scaling factor trong trường hợp này là \( \times 2 \) ). Bài toán Super Resolution được đặt ra như sau: cho trước một cặp ảnh low resolution và high resolution, tìm một thuật toán (có thể hiểu là model nếu bạn quen với Deep Learning nhiều hơn) nhận vào ảnh low resolution và trả về kết quả giống ảnh high resolution cho trước nhất. &ldquo;Giống nhất&rdquo; ở đây có thể được đo bằng nhiều metrics khác nhau, đơn giản nhất thì có thể dùng Mean Square Error giữa hai ảnh.
Vấn đề đặt ra ở đây là ảnh low resolution được sinh ra từ ảnh high resolution như thế nào? Cách tiếp cận phổ biến nhất (qua những papers Super Resolution mà mình đã đọc) là sử dụng Gaussian Blur lên trên ảnh High Resolution, sau đó dùng bicubic interpolation để resize (thu nhỏ) lại và cuối cùng cộng với một Gaussian noise. Thông thường, quá trình đó được thể hiện thông qua công thức:</p>
<p>$$I^{LR} = {(I^{HR} \circledast k) \downarrow} + \mathcal{N} $$</p>
<p>Trong đó:</p>
<ul>
<li>\( \circledast \) là convoluton operator</li>
<li>\( k \) là Guassian kernel dùng để blur (cái này quan trọng, nhớ nó nhé)</li>
<li>\(  \downarrow \) là bicubic interpolation</li>
<li>\(  \mathcal{N} \) là Gaussian noise</li>
</ul>
<p>Nói một cách đơn giản, thì Super Resolution, đặc biệt nếu dùng cách tiếp cận bằng Deep Learning models, thì sẽ cố gắng đi tìm &ldquo;hàm ngược&rdquo; \( k' \) của kernel \( k \).
Mình đã &ldquo;look over&rdquo; chi tiết này, nhưng cũng nhờ vậy mà mình có được những bài học.</p>
<p>Deep Learning models thường được ưa chuộng vì khả năng generalization của nó nếu được trained properly và mình nghĩ Super Resolution models cũng vậy. Trong lúc làm thí nghiệm, mình có một xây dựng một vài models cơ bản. Cơ bản ở đây theo nghĩa là nó đơn giản để xây dựng dựa trên một số công trình ở trước, chứ không hẳn là nó đơn giản thật =))). Mình có thử 2 cái, mà mình đặt tên là SimpleBaseline và DumbNet (đúng rồi, cái DumbNet mà mình đã viết <a href="https://incenger.github.io/post/fellowship_optim/random_things_2/">ở đây</a><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.). Kết quả train của 2 models này, trên tập validation (gồm 4 videos) của dataset REDS khá là tốt, đạt được khoảng 30 PSNR score (ngang bằng với đa số papers trong khoảng vài năm trở lại đây, nhưng tất nhiên không phải cao nhất =))) ).</p>
<p>Với kết quả như vậy, thì việc tiếp theo mình sẽ làm là sẽ test thử kết quả trên một dataset khác. Dataset đó sẽ đóng vai trò là một out-of-scope test set (hoặc out-of-distribution, mình không nhớ chính xác tên khái niệm lắm) - một tập test mà data sẽ khác hoàn toàn với data mà model được train (khác ở đây theo nghĩ chúng có thể được sampled từ những distribution khác nhau, nếu muốn tìm hiểu thêm về phần này, mình recommend các bạn nên đọc quyển <a href="https://www.manning.com/books/human-in-the-loop-machine-learning">Human-in-the-Loop Machine Learning</a><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>). Mình sử dụng dataset Vid4 -  một dataset khác cũng được sử dụng khá phổ biến cho bài toán Video Super Resolution. Dataset này thì không có sẵn low resolution frame, nên mình viết code để tạo chúng từ high resolution frames. Sau khi test thử, kết quả mình nhận được rất &hellip; tệ, tệ hơn hẳn việc sử dụng bicubic interpolation thông thường.  Và thế là quá trình debug của mình bắt đầu.</p>
<ol>
<li>Để debug trong deep learning, các bạn không nên chỉ nhìn vào con số tổng quát mà hãy tập trung vào từng sample cụ thể. Mình xem kết quả super resoluton frame by frame và nhận ra là output của mình có rất nhiều artifact như thế này.</li>
</ol>
<p><img src="/img/2021/you_dont_know_3/vid4_artifact.png" alt="Kết quả rất tệ&hellip;"></p>
<ol start="2">
<li>
<p>Mình không thể hiểu được kết quả. Đối với các bài toán như Classification, việc hiểu kết quả đưa ra bởi model sẽ trực quan hơn nhiều. Còn với Super Resolution, mình không thật sự hiểu được hoàn toàn mà mình chỉ biết là nó tệ hay tốt thôi. Dù vậy, mình có một giả thiết là việc generalization của model có vấn đề.</p>
</li>
<li>
<p>Mình kiểm tra lại trong paper, thì không thấy paper nào nói về chuyện này cả, và những papers đó đều có sử dụng Vid4 dataset để evaluate và kết quả khá tốt, ít nhất là tốt hơn bicubic interpolation. Mình nghĩ có thể có 2 trường hợp xảy ra: một là model của mình overfit data và hai là model đơn giản quá nên underfit. Sau khi kiểm tra kết quả trên chính training set, mình khá chắc chắn đây không thể là overfitting vì kết quả không quá tốt, do vậy mình kết luận là underfitting.</p>
</li>
<li>
<p>Mình làm thí nghiệm để xác thực giả thuyết underfitting. Mình đo số parameters và đồng thời chỉnh lại hyperparameters của model để tăng complexity lên. Sau khi train lại và evaluate lại, mình nhận kết quả tương tự :(. Và thế là mình bị mắc kẹt.</p>
</li>
<li>
<p>Lúc này, mình đành phải kiểm chứng lại kết quả của những papers khác. Mình sử dụng code và pre-trained models của họ để chạy thử trên data Vid4 của mình. Và thật bất ngờ, kết quả cũng tệ tương tự.</p>
</li>
<li>
<p>Tới lúc này, giả thuyết của mình chuyển từ underfitting sang việc generalization trong bài toán Super Resoluton có gì đó mà mình chưa biết (yeah, you don&rsquo;t know what you don&rsquo;t know :))) ). MÌnh bắt đầu đi tìm hiểu thử về vấn đề này. Mình bắt gặp một <a href="https://github.com/open-mmlab/mmediting/issues/59">Github issue</a><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> và sau khi đọc nó mình đã hiểu ra vấn đề.</p>
</li>
</ol>
<p>Thật ra việc generalization của Super Resolution không có vấn đề gì cả, chỉ là mình hiểu sai nó. Với đa số cách tiếp cận hiên tại, với input chỉ gồm ảnh low resolution, models có thể generalize trên những image domain khác, nhưng không thể generalize trên những downsample kernel \(  k \) khác nhau tốt được. Và bằng việc tự mình generate low resolution frames cho Vid4 dataset, mình đã dùng một downsample kernel khác với downsample kernel mà model đã được optimized. Điều đó giải thích cho việc tại sao kết quả của mình không tốt. Ngoài ra, mình còn biết được thêm là có một nhánh nghiên cứu Super Resolution mà models có thể generalize với nhiều downsample kernels khác nhau,  gọi là Blind Super Resolution. Khi đó input cho model khi train sẽ cần thêm downsample kernel đã được dùng để sinh ra ảnh low resolution. Okay, vấn đề đã được giải quyết, mình tìm code mà đã được dùng cho dataset REDS và sử dụng nó cho dataset Vid4 và kết quả mình nhận được make sense hơn rất nhiều.</p>
<p>Bên cạnh việc hiểu được vấn đề, sự phát hiện này cũng làm mình hơi hoang mang một tí vì bất ngờ mình có thêm một hướng nghiên cứu cần phải để tâm. Nhưng đó sẽ là một câu chuyện khác và mình sẽ kể khi mình bớt hoang mang hơn :))). Thường sau khi giải quyết xong vấn đề, mình thường dành thời gian để reflection - nhìn lại và nghĩ xem mình có thể làm tốt hơn ở những bước nào.</p>
<p>Thật ra, trong quá trình mình giải quyết vấn đề ở trên, nếu mình bắt đầu từ bước 6 thì sẽ đỡ tốn thời gian hơn rất nhiều. Lúc mình đang phân vân giữa 2 giả thuyết overfitting và underfitting, mình cũng đã nghĩ đến generalization. Nhìn lại, mình nhận ra mình chọn 2 giả thuyết kia để thử vì việc xác thực tụi nó dễ với mình nhiều hơn là mình nghĩ chúng có thể là cốt lõi của vấn đề. Mình không tìm hiểu về generalization đầu tiên vì mình không có nhiều manh mối về nó và nó sẽ có thể làm lay chuyển những assumptions mà mình đang sử dụng trước giờ. Và đây là một bài học cho mình: trong những lựa chọn, không nên lúc nào cũng ưu tiên những lựa chọn trong &ldquo;comfort zone&rdquo; của mình.</p>
<p>Ngoài ra, trong lúc tìm hiểu về Blind Super Resolution, mình tìm thấy đươc một paper có show kết quả super resolution khi mà kernel được dùng để downsample và kernel mà model học được khác nhau. Điều này giúp mình hiểu rõ hơn về kết quả thí nghiệm của mình -  điều mà trước đây luôn làm mình bối rối. Mình cũng thấy khá thắc mắc, là hầu như trong những papers mình đọc qua, không có paper nào nhắc qua về những kết quả kiểu như thế này. Thật ra mình cũng không thấy bất ngờ lắm, vì mình nghĩ hầu như các papers cũng sẽ đều tập trung vào kết quả tốt và bỏ đi những kết quả &ldquo;thất bại&rdquo;. Mình cũng từng đọc qua điều này trong quyển <a href="https://www.goodreads.com/book/show/18693884-how-not-to-be-wrong">How Not to Be Wrong: The Power of Mathematical Thinking</a><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, trong đó tác giả đề xuất rằng những thí nghiệm thất bại nên được coi trọng hơn và được đưa vào trong những papers nhiều hơn. Mình nghĩ lúc mình viết thesis, mình cũng sẽ cố đưa những kết quả thí nghiệm dù thất bại nhưng vẫn có giá trị vào :)).</p>
<p>Vừa rồi là những suy nghĩ linh tinh của mình. Hẹn gặp lại các bạn trong những bài viết tiếp theo  \ (•◡•) /.</p>
<hr>
<blockquote>
<p>Các bạn có thể tìm đọc tất cả bài viết của series này tại <a href="https://incenger.github.io/categories/fellowship-optim/">đây</a>.</p>
</blockquote>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://incenger.github.io/post/fellowship_optim/random_things_2/">https://incenger.github.io/post/fellowship_optim/random_things_2/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://www.manning.com/books/human-in-the-loop-machine-learning">https://www.manning.com/books/human-in-the-loop-machine-learning</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://github.com/open-mmlab/mmediting/issues/59">https://github.com/open-mmlab/mmediting/issues/59</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://www.goodreads.com/book/show/18693884-how-not-to-be-wrong">https://www.goodreads.com/book/show/18693884-how-not-to-be-wrong</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
            </div>
        </article>

        <hr />

        <div class="post-info">
            
            
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-folder meta-icon"><path d="M22 19a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h5l2 3h9a2 2 0 0 1 2 2z"></path></svg>

        <span class="tag"><a href="/categories/fellowship-optim/">Fellowship-Optim</a></span>
        
    </p>

  		</div>
    </main>

            </div>

            
                <footer class="footer">
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2022</span>
            <span><a href="/">Tam Le</a></span>
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            <span><a href="/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
            
        </div>
    </div>
    
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by <a href="http://gohugo.io">Hugo</a></span><span>Theme by <a href="https://github.com/rhazdon">Djordje Atlialp</a></span>
        </div>
    </div>
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.599099f1f14b78b657d524b28e10e0c5098e7cd46e9c7aed73d577068a276c3ff1bb234cbf29cb313333e83cf411727b43157c91ce5b809e2ffc81664614608e.js" integrity="sha512-WZCZ8fFLeLZX1SSyjhDgxQmOfNRunHrtc9V3BoonbD/xuyNMvynLMTMz6Dz0EXJ7QxV8kc5bgJ4v/IFmRhRgjg=="></script>



    </body>
</html>
